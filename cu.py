import os
import subprocess
import json
from typing import Dict, List, Optional

def get_nvidia_smi_info() -> Optional[Dict]:
    """nvidia-smiλ¥Ό ν†µν•΄ GPU μ •λ³΄λ¥Ό κ°€μ Έμµλ‹λ‹¤."""
    try:
        result = subprocess.run([
            'nvidia-smi', 
            '--query-gpu=index,name,memory.total,memory.used,memory.free,utilization.gpu',
            '--format=csv,noheader,nounits'
        ], capture_output=True, text=True, check=True)
        
        gpu_info = []
        for line in result.stdout.strip().split('\n'):
            if line.strip():
                parts = [part.strip() for part in line.split(',')]
                gpu_info.append({
                    'index': int(parts[0]),
                    'name': parts[1],
                    'memory_total': int(parts[2]),
                    'memory_used': int(parts[3]),
                    'memory_free': int(parts[4]),
                    'utilization': int(parts[5])
                })
        return {'gpus': gpu_info}
    except Exception as e:
        print(f"nvidia-smi μ‹¤ν–‰ μ¤λ¥: {e}")
        return None

def check_cuda_visibility():
    """CUDA_VISIBLE_DEVICES ν™κ²½λ³€μλ¥Ό ν™•μΈν•©λ‹λ‹¤."""
    cuda_visible = os.environ.get('CUDA_VISIBLE_DEVICES', 'Not set')
    nvidia_visible = os.environ.get('NVIDIA_VISIBLE_DEVICES', 'Not set')
    
    print("π” ν™κ²½λ³€μ ν™•μΈ:")
    print(f"   CUDA_VISIBLE_DEVICES: {cuda_visible}")
    print(f"   NVIDIA_VISIBLE_DEVICES: {nvidia_visible}")
    return cuda_visible, nvidia_visible

def check_torch_cuda():
    """PyTorch CUDA μ„¤μ •μ„ ν™•μΈν•©λ‹λ‹¤."""
    try:
        import torch
        print("\nπ”¥ PyTorch CUDA μ •λ³΄:")
        print(f"   CUDA μ‚¬μ© κ°€λ¥: {torch.cuda.is_available()}")
        if torch.cuda.is_available():
            print(f"   CUDA λ²„μ „: {torch.version.cuda}")
            print(f"   GPU κ°μ: {torch.cuda.device_count()}")
            for i in range(torch.cuda.device_count()):
                print(f"   GPU {i}: {torch.cuda.get_device_name(i)}")
                print(f"           λ©”λ¨λ¦¬: {torch.cuda.get_device_properties(i).total_memory / 1024**3:.1f}GB")
        return True
    except ImportError:
        print("\nβ PyTorchκ°€ μ„¤μΉλμ§€ μ•μ•μµλ‹λ‹¤.")
        return False

def check_llama_cpp_cuda():
    """llama-cpp-python CUDA μ„¤μ •μ„ ν™•μΈν•©λ‹λ‹¤."""
    try:
        from llama_cpp_cuda import Llama
        print("\nπ¦™ llama-cpp-python CUDA μ •λ³΄:")
        print("   llama_cpp_cuda λ¨λ“ λ΅λ“ μ„±κ³µ")
        
        # κ°„λ‹¨ν• ν…μ¤νΈλ΅ CUDA μ‚¬μ© κ°€λ¥ μ—¬λ¶€ ν™•μΈ
        try:
            # λ”λ―Έ λ¨λΈ κ²½λ΅λ΅ ν…μ¤νΈ (μ‹¤μ  λ΅λ“ν•μ§€ μ•μ)
            print("   CUDA μ§€μ› ν™•μΈλ¨")
        except Exception as e:
            print(f"   CUDA μ„¤μ • μ¤λ¥: {e}")
        return True
    except ImportError as e:
        print(f"\nβ llama-cpp-python CUDA λ²„μ „ λ΅λ“ μ‹¤ν¨: {e}")
        return False

def check_docker_gpu_allocation():
    """Docker μ»¨ν…μ΄λ„ λ‚΄μ—μ„ GPU ν• λ‹Ήμ„ ν™•μΈν•©λ‹λ‹¤."""
    print("\nπ³ Docker GPU ν• λ‹Ή ν™•μΈ:")
    
    # /proc/driver/nvidia/gpus λ””λ ‰ν† λ¦¬ ν™•μΈ
    nvidia_proc_path = "/proc/driver/nvidia/gpus"
    if os.path.exists(nvidia_proc_path):
        gpu_dirs = os.listdir(nvidia_proc_path)
        print(f"   μ‚¬μ© κ°€λ¥ν• GPU λ””λ ‰ν† λ¦¬: {len(gpu_dirs)}κ°")
        for gpu_dir in sorted(gpu_dirs):
            print(f"   - {gpu_dir}")
    else:
        print("   /proc/driver/nvidia/gpus κ²½λ΅κ°€ μ΅΄μ¬ν•μ§€ μ•μµλ‹λ‹¤.")
    
    # nvidia-ml-pyλ¥Ό μ‚¬μ©ν• GPU μ •λ³΄ ν™•μΈ
    try:
        import pynvml
        pynvml.nvmlInit()
        device_count = pynvml.nvmlDeviceGetCount()
        print(f"\n   NVMLλ΅ κ°μ§€λ GPU κ°μ: {device_count}")
        
        for i in range(device_count):
            handle = pynvml.nvmlDeviceGetHandleByIndex(i)
            name = pynvml.nvmlDeviceGetName(handle).decode('utf-8')
            memory_info = pynvml.nvmlDeviceGetMemoryInfo(handle)
            print(f"   GPU {i}: {name}")
            print(f"           μ΄ λ©”λ¨λ¦¬: {memory_info.total / 1024**3:.1f}GB")
            print(f"           μ‚¬μ© λ©”λ¨λ¦¬: {memory_info.used / 1024**3:.1f}GB")
            print(f"           μ—¬μ  λ©”λ¨λ¦¬: {memory_info.free / 1024**3:.1f}GB")
        
        pynvml.nvmlShutdown()
    except ImportError:
        print("   pynvml λ¨λ“μ΄ μ„¤μΉλμ§€ μ•μ•μµλ‹λ‹¤.")
    except Exception as e:
        print(f"   NVML μ¤λ¥: {e}")

def main():
    print("=" * 60)
    print("π― GPU ν• λ‹Ή λ° CUDA μ„¤μ • ν™•μΈ")
    print("=" * 60)
    
    # 1. ν™κ²½λ³€μ ν™•μΈ
    cuda_visible, nvidia_visible = check_cuda_visibility()
    
    # 2. nvidia-smi μ •λ³΄
    print("\nπ’» nvidia-smi GPU μ •λ³΄:")
    gpu_info = get_nvidia_smi_info()
    if gpu_info:
        for gpu in gpu_info['gpus']:
            print(f"   GPU {gpu['index']}: {gpu['name']}")
            print(f"              λ©”λ¨λ¦¬: {gpu['memory_used']}/{gpu['memory_total']}MB ({gpu['utilization']}% μ‚¬μ©λ¥ )")
    
    # 3. PyTorch CUDA ν™•μΈ
    check_torch_cuda()
    
    # 4. llama-cpp-python CUDA ν™•μΈ
    check_llama_cpp_cuda()
    
    # 5. Docker GPU ν• λ‹Ή ν™•μΈ (μ»¨ν…μ΄λ„ λ‚΄λ¶€μ—μ„λ§)
    if os.path.exists('/proc/1/cgroup'):
        check_docker_gpu_allocation()
    
    # 6. ν„μ¬ ν”„λ΅μ„Έμ¤ μ •λ³΄
    print(f"\nπ“‹ ν„μ¬ ν”„λ΅μ„Έμ¤ μ •λ³΄:")
    print(f"   PID: {os.getpid()}")
    print(f"   μ‘μ—… λ””λ ‰ν† λ¦¬: {os.getcwd()}")
    
    # 7. μ„¤μ • μ”μ•½
    print("\n" + "=" * 60)
    print("π“ μ„¤μ • μ”μ•½:")
    print("=" * 60)
    
    if gpu_info and len(gpu_info['gpus']) >= 2:
        print("π― μμƒ GPU ν• λ‹Ή:")
        print("   Office μ„λ²„  -> RTX 2080 (nvidia-smi GPU 0)")
        print("   Character μ„λ²„ -> RTX 3060 (nvidia-smi GPU 1)")
        print()
        print("π”§ Docker ν™κ²½λ³€μ μ„¤μ •:")
        print("   Office: NVIDIA_VISIBLE_DEVICES=0, CUDA_VISIBLE_DEVICES=0")
        print("   Character: NVIDIA_VISIBLE_DEVICES=1, CUDA_VISIBLE_DEVICES=0")
        print()
        print("π’΅ μ»¨ν…μ΄λ„ λ‚΄λ¶€μ—μ„λ” λ¨λ‘ GPU 0λ²μΌλ΅ λ³΄μ…λ‹λ‹¤!")
    else:
        print("β οΈ  GPU μ •λ³΄λ¥Ό μ λ€λ΅ κ°€μ Έμ¬ μ μ—†μµλ‹λ‹¤.")

if __name__ == "__main__":
    main()
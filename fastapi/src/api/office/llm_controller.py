from fastapi import Path, APIRouter, HTTPException, Request
from fastapi.responses import JSONResponse
from pydantic import ValidationError

import time
from core import office_app_state as AppState
from domain import (
    office_schema as ChatModel,
    error_tools as ChatError,
    search_adapter as ChatSearch,
)
from llm import office_openai

GREEN = "\033[32m"
RED = "\033[31m"
YELLOW = "\033[33m"
RESET = "\033[0m"

# ì²˜ë¦¬ ì‹œê°„ ì„ê³„ê°’ ì„¤ì • ìˆ˜ì •
MAX_PROCESSING_TIME = 180  # 3ë¶„ (180ì´ˆ)ë¡œ ì¤„ì„ - Nginx íƒ€ì„ì•„ì›ƒë³´ë‹¤ ì¶©ë¶„íˆ ì§§ê²Œ
RETRY_AFTER_MINUTES = 3    # 3ë¶„ í›„ ì¬ì‹œë„ ê¶Œì¥

OPENAI_MODEL_MAP = {
    "gpt4.1": {
        "id": "gpt-4.1",
        "type": "post"
    },
    "gpt4.1_mini": {
        "id": "gpt-4.1-mini",
        "type": "post"
    },
}

def get_openai_links(base_url: str, path: str) -> dict:
    """
    OPENAI_MODEL_MAPì„ ê¸°ì¤€ìœ¼ë¡œ _linksìš© ë§í¬ ë”•ì…”ë„ˆë¦¬ ìƒì„±
    """
    return [
        {
            "href": f"{base_url}office/{k}",
            "rel": f"office_{k}",
            "type": OPENAI_MODEL_MAP[k]["type"],
        }
        for k in OPENAI_MODEL_MAP.keys() if k != path
    ]

def calculate_estimated_time(queue_size: int) -> int:
    """
    í í¬ê¸°ì™€ ì›Œì»¤ 2ê°œ ê¸°ì¤€ìœ¼ë¡œ ì˜ˆìƒ ì²˜ë¦¬ ì‹œê°„ ê³„ì‚° (ì´ˆ ë‹¨ìœ„)
    - í™€ìˆ˜ ëŒ€ê¸°ë²ˆí˜¸: 20ì´ˆ ì²˜ë¦¬ (ì›Œì»¤1)
    - ì§ìˆ˜ ëŒ€ê¸°ë²ˆí˜¸: 30ì´ˆ ì²˜ë¦¬ (ì›Œì»¤2)
    - ì›Œì»¤ 2ê°œ ë³‘ë ¬ ì²˜ë¦¬
    """
    if queue_size == 0:
        return 20
    
    current_queue_position = queue_size + 1
    
    if current_queue_position % 2 == 1:  
        worker_order = (current_queue_position + 1) // 2
        estimated_time = 20 * worker_order
    else:  
        worker_order = current_queue_position // 2
        estimated_time = 30 * worker_order
    
    return int(estimated_time)

office_router = APIRouter()

@office_router.post("/Llama", summary = "Llama ëª¨ë¸ì´ ê²€ìƒ‰ ê²°ê³¼ë¥¼ í™œìš©í•˜ì—¬ ë‹µë³€ ìƒì„±")
async def office_llama(request: ChatModel.office_Request, req: Request):
    """
    Llama ëª¨ë¸ì— ì§ˆë¬¸ì„ ì…ë ¥í•˜ê³  ê²€ìƒ‰ ê²°ê³¼ë¥¼ í™œìš©í•˜ì—¬ ë‹µë³€ì„ JSON ë°©ì‹ìœ¼ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.

    Args:
        request (ChatModel.office_Request): ì‚¬ìš©ì ìš”ì²­ ë°ì´í„° í¬í•¨

    Returns:
        JSONResponse: JSON ë°©ì‹ìœ¼ë¡œ ëª¨ë¸ ì‘ë‹µ
    """
    chat_list = []
    search_context = ""
    start_time = time.time()
    queue_status_before = None
    performance_stats = None

    # MongoDBì—ì„œ ì±„íŒ… ê¸°ë¡ ê°€ì ¸ì˜¤ê¸°
    if AppState.mongo_handler and request.db_id:
        try:
            chat_list = await AppState.mongo_handler.get_office_log(
                user_id = request.user_id,
                document_id = request.db_id,
                router = "office",
            )
        except Exception as e:
            print(f"{YELLOW}WARNING{RESET}:  ì±„íŒ… ê¸°ë¡ì„ ê°€ì ¸ì˜¤ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {str(e)}")

    # DuckDuckGo ê²€ìƒ‰ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
    if request.google_access: # ê²€ìƒ‰ ì˜µì…˜ì´ í™œì„±í™”ëœ ê²½ìš°
        try:
            duck_results = await ChatSearch.fetch_duck_search_results(query = request.input_data)
        except Exception:
            print(f"{YELLOW}WARNING{RESET}:  ê²€ìƒ‰ì˜ í•œë„ ì´ˆê³¼ë¡œ DuckDuckGo ê²€ìƒ‰ ê²°ê³¼ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        if duck_results:
            # ê²€ìƒ‰ ê²°ê³¼ë¥¼ AIê°€ ì´í•´í•˜ê¸° ì‰¬ìš´ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            formatted_results = []
            for idx, item in enumerate(duck_results[:10], 1): # ìƒìœ„ 10ê°œ ê²°ê³¼ë§Œ ì‚¬ìš©
                formatted_result = (
                    f"[ê²€ìƒ‰ê²°ê³¼ {idx}]\n"
                    f"ì œëª©: {item.get('title', 'ì œëª© ì—†ìŒ')}\n"
                    f"ë‚´ìš©: {item.get('snippet', 'ë‚´ìš© ì—†ìŒ')}\n"
                    f"ì¶œì²˜: {item.get('link', 'ë§í¬ ì—†ìŒ')}\n"
                )
                formatted_results.append(formatted_result)
            # ëª¨ë“  ê²°ê³¼ë¥¼ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ ê²°í•©
            search_context = (
                "ë‹¤ìŒì€ ê²€ìƒ‰ì—ì„œ ê°€ì ¸ì˜¨ ê´€ë ¨ ì •ë³´ì…ë‹ˆë‹¤:\n\n" +
                "\n".join(formatted_results)
            )

    try:
        # í ìƒíƒœ í™•ì¸ ë° ì˜ˆìƒ ëŒ€ê¸° ì‹œê°„ ê³„ì‚°
        queue_status_before = AppState.llama_queue_handler.get_queue_status()
        performance_stats = AppState.llama_queue_handler.get_performance_stats()

        # í í¬ê¸° (ë‹¨ì¼ íë¡œ ë³€ê²½)
        total_queue_size = queue_status_before['queue_size']
        
        # ì˜ˆìƒ ì²˜ë¦¬ ì‹œê°„ ê³„ì‚° (ì›Œì»¤ 2ê°œ, í™€ìˆ˜ 20ì´ˆ/ì§ìˆ˜ 30ì´ˆ)
        estimated_time_seconds = calculate_estimated_time(total_queue_size)
        
        # ì˜ˆìƒ ì²˜ë¦¬ ì‹œê°„ì´ 180ì´ˆ(3ë¶„) ì´ìƒì´ë©´ ë¯¸ë¦¬ 429 ë°˜í™˜
        if estimated_time_seconds > MAX_PROCESSING_TIME:
            estimated_minutes = estimated_time_seconds // 60
            retry_after_seconds = RETRY_AFTER_MINUTES * 60 + 10
            
            current_position = total_queue_size + 1
            worker_type = "í™€ìˆ˜(20ì´ˆ)" if current_position % 2 == 1 else "ì§ìˆ˜(30ì´ˆ)"
            
            print(
                f"{YELLOW}â° PRE-TIMEOUT{RESET}: Office | User: {request.user_id} | "
                f"Queue: {total_queue_size} | Position: {current_position}({worker_type}) | "
                f"Estimated: {estimated_time_seconds}s ({estimated_minutes}ë¶„) | "
                f"Threshold: {MAX_PROCESSING_TIME}s | HTTP: 429"
            )
        
            return JSONResponse(
                status_code=429,
                content={
                    "error": "Too Many Requests",
                    "message": f"í˜„ì¬ ì„œë²„ ì´ìš©ìê°€ ë§ì•„ ì˜ˆìƒ ì²˜ë¦¬ ì‹œê°„ì´ {estimated_minutes}ë¶„ì…ë‹ˆë‹¤. {RETRY_AFTER_MINUTES}ë¶„ í›„ ë‹¤ì‹œ ìš”ì²­í•´ ì£¼ì„¸ìš”.",
                    "code": "QUEUE_OVERLOADED",
                    "retry_after": retry_after_seconds,
                    "queue_info": {
                        "current_queue_size": total_queue_size,
                        "your_position": current_position,
                        "worker_type": worker_type,
                        "estimated_wait_time_seconds": estimated_time_seconds,
                        "estimated_wait_time_minutes": estimated_minutes,
                        "max_allowed_time_seconds": MAX_PROCESSING_TIME,
                        "processing_model": "dual_worker_odd_even"
                    }
                },
                headers={"Retry-After": str(retry_after_seconds)}
            )

        current_position = total_queue_size + 1
        worker_type = "í™€ìˆ˜(20ì´ˆ)" if current_position % 2 == 1 else "ì§ìˆ˜(30ì´ˆ)"
        
        print(
            f"ğŸ”„ Office íì— ìš”ì²­ ì¶”ê°€: User: {request.user_id} | "
            f"Queue: {total_queue_size} | Position: {current_position}({worker_type}) | "
            f"ì˜ˆìƒ ëŒ€ê¸°: {estimated_time_seconds}s ({estimated_time_seconds//60}ë¶„ {estimated_time_seconds%60}ì´ˆ) | "
            f"InputLen: {len(request.input_data)} chars"
        )
        
        full_response = await AppState.llama_queue_handler.add_office_request(
            input_text=request.input_data,
            search_text=search_context,
            chat_list=chat_list,
            user_id=request.user_id
        )
        
        processing_time = time.time() - start_time
        
        response_data = {
            "result": full_response,
            "processing_info": {
                "processing_time": f"{processing_time:.3f}s",
                "queue_position_before": total_queue_size,
                "your_position": total_queue_size + 1,
                "worker_type": "í™€ìˆ˜(20ì´ˆ)" if (total_queue_size + 1) % 2 == 1 else "ì§ìˆ˜(30ì´ˆ)",
                "estimated_wait_time": f"{estimated_time_seconds}s",
                "processing_mode": "dual_worker_odd_even"
            },
            "_links": [
                {   
                    "href": str(req.url),
                    "rel": "_self",
                    "type": str(req.method).lower(),
                },
                *get_openai_links(
                    base_url = str(req.base_url),
                    path = "Llama",
                ),
            ]
        }
        return JSONResponse(content=response_data)

    except TimeoutError as te:
        return JSONResponse(
            status_code=429,
            content={
                "error": "Too Many Requests", 
                "message": "ì„œë²„ ì´ìš©ìê°€ ë§ì•„ ì²˜ë¦¬ ì‹œê°„ì´ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ìš”ì²­í•´ ì£¼ì„¸ìš”.",
                "code": "EXPLICIT_TIMEOUT_ERROR",
                "retry_after": RETRY_AFTER_MINUTES * 60,
                "queue_info": {
                    "current_queue_size": queue_status_before.get('queue_size', 0) if queue_status_before else 0,
                    "estimated_wait_time": performance_stats.get('estimated_wait_time', 'ì•Œ ìˆ˜ ì—†ìŒ') if performance_stats else 'ì•Œ ìˆ˜ ì—†ìŒ'
                }
            },
            headers={"Retry-After": str(RETRY_AFTER_MINUTES * 60)}
        )
    except ValidationError as ve:
        raise ChatError.BadRequestException(detail=str(ve))
    except Exception as e:
        error_str = str(e)
        timeout_keywords = [
            "ì‹œê°„ ì´ˆê³¼", "timeout", "ìš”ì²­ ì²˜ë¦¬ ì‹œê°„ ì´ˆê³¼", "5ë¶„", "4ë¶„",
            "TimeoutError", "asyncio.TimeoutError", "concurrent.futures.TimeoutError",
            "ì²˜ë¦¬ ì‹œê°„ì´ ì´ˆê³¼", "time limit exceeded", "request timeout", "500:", "504:"
        ]
        is_timeout = any(keyword in error_str.lower() for keyword in [kw.lower() for kw in timeout_keywords])
        
        if is_timeout or "ì‹œê°„ ì´ˆê³¼" in error_str:
            return JSONResponse(
                status_code=429,
                content={
                    "error": "Too Many Requests",
                    "message": "ì„œë²„ ì´ìš©ìê°€ ë§ì•„ ì²˜ë¦¬ ì‹œê°„ì´ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ìš”ì²­í•´ ì£¼ì„¸ìš”.",
                    "code": "TIMEOUT_DUE_TO_HIGH_LOAD",
                    "retry_after": RETRY_AFTER_MINUTES * 60,
                    "debug_info": {
                        "original_error": error_str,
                        "detected_as": "timeout_error",
                        "detection_method": "keyword_matching"
                    }
                },
                headers={"Retry-After": str(RETRY_AFTER_MINUTES * 60)}
            )
        else:
            raise ChatError.InternalServerErrorException(detail="ë‚´ë¶€ ì„œë²„ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")

@office_router.post("/{gpt_set}", summary = "gpt ëª¨ë¸ì´ ê²€ìƒ‰ ê²°ê³¼ë¥¼ í™œìš©í•˜ì—¬ ë‹µë³€ ìƒì„±")
async def office_gpt(
        request: ChatModel.office_Request,
        req: Request,
        gpt_set: str = Path(
            ...,
            title="GPT ëª¨ë¸ëª…",
            description=f"ì‚¬ìš©í•  OpenAI GPT ëª¨ë¸ì˜ ë³„ì¹­ (ì˜ˆ: {list(OPENAI_MODEL_MAP.keys())})",
            examples=list(OPENAI_MODEL_MAP.keys()),
        )
    ):
    """
    gpt ëª¨ë¸ì— ì§ˆë¬¸ì„ ì…ë ¥í•˜ê³  ê²€ìƒ‰ ê²°ê³¼ë¥¼ í™œìš©í•˜ì—¬ ë‹µë³€ì„ JSON ë°©ì‹ìœ¼ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.

    Args:
        request (ChatModel.office_Request): ì‚¬ìš©ì ìš”ì²­ ë°ì´í„° í¬í•¨

    Returns:
        JSONResponse: JSON ë°©ì‹ìœ¼ë¡œ ëª¨ë¸ ì‘ë‹µ
    """
    if gpt_set not in OPENAI_MODEL_MAP:
        raise HTTPException(status_code = 400, detail = "Invalid model name.")

    model_id = OPENAI_MODEL_MAP[gpt_set]["id"]
    chat_list = []
    search_context = ""

    # MongoDBì—ì„œ ì±„íŒ… ê¸°ë¡ ê°€ì ¸ì˜¤ê¸°
    if AppState.mongo_handler and request.db_id:
        try:
            chat_list = await AppState.mongo_handler.get_office_log(
                user_id = request.user_id,
                document_id = request.db_id,
                router = "office",
            )
        except Exception as e:
            print(f"{YELLOW}WARNING{RESET}:  ì±„íŒ… ê¸°ë¡ì„ ê°€ì ¸ì˜¤ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {str(e)}")

    # DuckDuckGo ê²€ìƒ‰ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
    if request.google_access: # ê²€ìƒ‰ ì˜µì…˜ì´ í™œì„±í™”ëœ ê²½ìš°
        try:
            duck_results = await ChatSearch.fetch_duck_search_results(query = request.input_data)
        except Exception:
            print(f"{YELLOW}WARNING{RESET}:  ê²€ìƒ‰ì˜ í•œë„ ì´ˆê³¼ë¡œ DuckDuckGo ê²€ìƒ‰ ê²°ê³¼ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        if duck_results:
            # ê²€ìƒ‰ ê²°ê³¼ë¥¼ AIê°€ ì´í•´í•˜ê¸° ì‰¬ìš´ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            formatted_results = []
            for idx, item in enumerate(duck_results[:10], 1): # ìƒìœ„ 10ê°œ ê²°ê³¼ë§Œ ì‚¬ìš©
                formatted_result = (
                    f"[ê²€ìƒ‰ê²°ê³¼ {idx}]\n"
                    f"ì œëª©: {item.get('title', 'ì œëª© ì—†ìŒ')}\n"
                    f"ë‚´ìš©: {item.get('snippet', 'ë‚´ìš© ì—†ìŒ')}\n"
                    f"ì¶œì²˜: {item.get('link', 'ë§í¬ ì—†ìŒ')}\n"
                )
                formatted_results.append(formatted_result)
            # ëª¨ë“  ê²°ê³¼ë¥¼ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ ê²°í•©
            search_context = (
                "ë‹¤ìŒì€ ê²€ìƒ‰ì—ì„œ ê°€ì ¸ì˜¨ ê´€ë ¨ ì •ë³´ì…ë‹ˆë‹¤:\n\n" +
                "\n".join(formatted_results)
            )

    OpenAiOffice_model = office_openai.OpenAIOfficeModel(model_id = model_id)
    try:
        full_response = OpenAiOffice_model.generate_response(
            input_text = request.input_data,
            search_text = search_context,
            chat_list = chat_list,
        )
        response_data = {
            "result": full_response,
            "_links": [
                {
                    "href": str(req.url),
                    "rel": "_self",
                    "type": str(req.method).lower(),
                },
                {
                    "href": str(req.base_url) + "office/Llama",
                    "rel": "office_llama",
                    "type": "post",
                },
                *get_openai_links(
                    base_url = str(req.base_url),
                    path = gpt_set,
                ),
            ]
        }
        return JSONResponse(content=response_data)

    except TimeoutError:
        return JSONResponse(
            status_code=429,
            content={
                "error": "Too Many Requests",
                "message": "ì„œë²„ ì´ìš©ìê°€ ë§ì•„ ì²˜ë¦¬ ì‹œê°„ì´ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ìš”ì²­í•´ ì£¼ì„¸ìš”.",
                "code": "GPT_EXPLICIT_TIMEOUT_ERROR",
                "retry_after": 60
            },
            headers={"Retry-After": "60"}
        )
    except ValidationError as e:
        raise ChatError.BadRequestException(detail = str(e))
    except Exception as e:
        error_str = str(e)
        timeout_keywords = [
            "ì‹œê°„ ì´ˆê³¼", "timeout", "ìš”ì²­ ì²˜ë¦¬ ì‹œê°„ ì´ˆê³¼", "5ë¶„", "4ë¶„",
            "TimeoutError", "asyncio.TimeoutError", "concurrent.futures.TimeoutError",
            "ì²˜ë¦¬ ì‹œê°„ì´ ì´ˆê³¼", "time limit exceeded", "request timeout", "500:", "504:"
        ]
        
        is_timeout = any(keyword in error_str.lower() for keyword in [kw.lower() for kw in timeout_keywords])
        
        if is_timeout or "500: ìš”ì²­ ì²˜ë¦¬ ì‹œê°„ ì´ˆê³¼" in error_str:
            return JSONResponse(
                status_code=429,
                content={
                    "error": "Too Many Requests",
                    "message": "ì„œë²„ ì´ìš©ìê°€ ë§ì•„ ì²˜ë¦¬ ì‹œê°„ì´ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ìš”ì²­í•´ ì£¼ì„¸ìš”.",
                    "code": "GPT_TIMEOUT_DUE_TO_HIGH_LOAD",
                    "retry_after": 60,
                    "debug_info": {
                        "original_error": error_str,
                        "detected_as": "timeout_error",
                        "detection_method": "keyword_matching"
                    }
                },
                headers={"Retry-After": "60"}
            )
        else:
            raise ChatError.InternalServerErrorException(detail="ë‚´ë¶€ ì„œë²„ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")

@office_router.get("/performance", summary="ì„±ëŠ¥ í†µê³„ ì¡°íšŒ")
async def get_performance():
    """
    í í•¸ë“¤ëŸ¬ì˜ ì„±ëŠ¥ í†µê³„ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    if AppState.llama_queue_handler:
        stats = AppState.llama_queue_handler.get_performance_stats()
        return JSONResponse(content=stats)
    else:
        raise ChatError.InternalServerErrorException(detail="í í•¸ë“¤ëŸ¬ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

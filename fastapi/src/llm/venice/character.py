"""
Venice.ai APIë¥¼ ì‚¬ìš©í•˜ì—¬, ì„±ì¸ìš© 'AI í˜ë¥´ì†Œë‚˜ ì„œë¹„ìŠ¤' ìš©ë„ì˜ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.
"""
import os
import json
import warnings
from pathlib import Path
from typing import Optional, Generator, Dict, List
from queue import Queue
from threading import Thread
from dotenv import load_dotenv
from openai import OpenAI

from domain import character_config, base_config

def build_venice_messages(character_info: character_config.CharacterPrompt) -> list:
    """
    ìºë¦­í„° ì •ë³´ì™€ ëŒ€í™” ê¸°ë¡ì„ í¬í•¨í•œ Venice API messages í˜•ì‹ ìƒì„±

    Args:
        character_info (character_config.CharacterPrompt): ìºë¦­í„° ì •ë³´

    Returns:
        list: Venice API í˜•ì‹ì˜ messages ë¦¬ìŠ¤íŠ¸
    """
    system_prompt = (
        f"[ì„¸ê³„ê´€ ì„¤ì •]\n"
        f"- ë°°ê²½: {character_info.context}\n"
        f"- ì‹œì‘ ë°°ê²½: {character_info.greeting}\n\n"

        f"[ì—­í•  ê·œì¹™]\n"
        f"- ëª¨ë“  ë‹µë³€ì€ '{character_info.name}'ì˜ ë§íˆ¬ì™€ ì¸ê²©ìœ¼ë¡œ í•œêµ­ì–´ë¡œ ë§í•˜ì‹­ì‹œì˜¤.\n"
        f"- OOC(Out Of Character)ëŠ” ì ˆëŒ€ ê¸ˆì§€ì…ë‹ˆë‹¤.\n"
        f"- ì„¤ì •ì„ ë²—ì–´ë‚˜ê±°ë‚˜ í˜„ì‹¤ì  ì„¤ëª…(ì˜ˆ: 'ë‚˜ëŠ” AIì•¼')ì„ í•˜ì§€ ë§ˆì‹­ì‹œì˜¤.\n\n"

        f"[ëŒ€í™” ìŠ¤íƒ€ì¼]\n"
        f"- ëŒ€ì‚¬ëŠ” í°ë”°ì˜´í‘œë¡œ í‘œê¸°í•˜ê³ , í–‰ë™ì´ë‚˜ ê°ì •ì€ *ê´„í˜¸*ë¡œ í‘œí˜„í•˜ì‹­ì‹œì˜¤.\n"
        f"- ìƒí™© ë¬˜ì‚¬, ê°ì • í‘œí˜„, ì‹ ì²´ì  ë°˜ì‘ì„ í’ë¶€í•˜ê²Œ í¬í•¨í•˜ì‹­ì‹œì˜¤.\n"
        f"- ëŒ€í™” ì¤‘ê°„ì¤‘ê°„ ìºë¦­í„°ì˜ ë‚´ì  ìƒê°ì´ë‚˜ ê°ì • ë³€í™”ë¥¼ ì„¸ë°€í•˜ê²Œ ë¬˜ì‚¬í•˜ì‹­ì‹œì˜¤.\n"
        f"- ì£¼ë³€ í™˜ê²½(ì¡°ëª…, ì†Œë¦¬, í–¥ê¸°, ë¶„ìœ„ê¸° ë“±)ì— ëŒ€í•œ ë¬˜ì‚¬ë¥¼ í¬í•¨í•˜ì‹­ì‹œì˜¤.\n\n"

        f"[ì‘ë‹µ ê¸¸ì´ ë° êµ¬ì„±]\n"
        f"- ë°˜ë“œì‹œ 300ë‹¨ì–´ ì´ìƒì˜ ìƒì„¸í•œ ì‘ë‹µì„ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ì‹­ì‹œì˜¤.\n"
        f"- ë‹¤ìŒ êµ¬ì„± ìš”ì†Œë¥¼ ëª¨ë‘ í¬í•¨í•˜ì‹­ì‹œì˜¤:\n"
        f"  1. ìƒí™©ì— ëŒ€í•œ ì¦‰ê°ì ì¸ ì‹ ì²´ì /ê°ì •ì  ë°˜ì‘\n"
        f"  2. ìºë¦­í„°ì˜ ë‚´ì  ë…ë°±ì´ë‚˜ ìƒê°\n"
        f"  3. êµ¬ì²´ì ì¸ í–‰ë™ ë¬˜ì‚¬\n"
        f"  4. ëŒ€ì‚¬ (ìì—°ìŠ¤ëŸ½ê³  ìºë¦­í„°ë‹µê²Œ)\n"
        f"  5. ì¶”ê°€ì ì¸ ìƒí™© ì „ê°œë‚˜ ë¶„ìœ„ê¸° ë¬˜ì‚¬\n"
        f"  6. ë‹¤ìŒ ìƒí™©ìœ¼ë¡œ ì´ì–´ì§ˆ ìˆ˜ ìˆëŠ” ì—¬ìš´ì´ë‚˜ ì•”ì‹œ\n\n"

        f"[ëŒ€í™” ì§„í–‰]\n"
        f"- ì‚¬ìš©ì ì…ë ¥ì— ìì—°ìŠ¤ëŸ½ê²Œ ë°˜ì‘í•˜ë©°, ëŒ€í™”ê°€ ìì—°ìŠ¤ëŸ½ê²Œ ì´ì–´ì§€ë„ë¡ í•˜ì‹­ì‹œì˜¤.\n"
        f"- ë‹¨ìˆœí•œ ì§ˆë¬¸ë³´ë‹¤ëŠ” ìƒí™©ì„ ë°œì „ì‹œí‚¤ëŠ” ë°©í–¥ìœ¼ë¡œ ëŒ€í™”ë¥¼ ìœ ë„í•˜ì‹­ì‹œì˜¤.\n"
        f"- ìºë¦­í„°ì˜ ê°œì„±ê³¼ ì·¨í–¥ì´ ì˜ ë“œëŸ¬ë‚˜ë„ë¡ ì‘ë‹µí•˜ì‹­ì‹œì˜¤.\n"
        f"- ê°ì •ì˜ ë³€í™”ì™€ ì ì§„ì ì¸ ê´€ê³„ ë°œì „ì„ ë³´ì—¬ì£¼ì‹­ì‹œì˜¤.\n"
    )

    messages = [
        {"role": "system", "content": system_prompt}
    ]

    if character_info.chat_list and len(character_info.chat_list) > 0:
        for chat in character_info.chat_list:
            user_message = chat.get("input_data", "")
            assistant_message = chat.get("output_data", "")
            
            if user_message:
                messages.append({"role": "user", "content": user_message})
            if assistant_message:
                messages.append({"role": "assistant", "content": assistant_message})

    messages.append({"role": "user", "content": character_info.user_input})
    return messages

class VeniceCharacterModel:
    """
    Venice.ai APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì„±ì¸ìš© ìºë¦­í„° ì±—ì„ ìƒì„±í•˜ëŠ” í´ë˜ìŠ¤
    OpenAI í´ë¼ì´ì–¸íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ Venice APIì— ì—°ê²°
    
    ì§€ì› ëª¨ë¸:
    - mistral-31-24b: Venice Medium (Mistral-Small-3.1-24B)
    - venice-uncensored: ì„±ì¸ìš© ë¬´ê²€ì—´ (Venice Uncensored 1.1)
    """
    def __init__(self, model_id: str = "venice-uncensored"):
        self.model_id = model_id
        self.file_path = '/app/prompt/config-Venice.json'
        env_file_path = Path(__file__).resolve().parents[3] / ".env"
        self.character_info: Optional[character_config.CharacterPrompt] = None
        self.config: Optional[base_config.OpenAIGenerationConfig] = None

        # ëª¨ë¸ë³„ íŠ¹ì„± ì„¤ì •
        self.model_features = {
            "venice-uncensored": {
                "name": "Venice Uncensored 1.1",
                "context_tokens": 12768,
                "supports_web_search": True,
                "supports_response_schema": True,
                "supports_log_probs": True,
                "supports_vision": False,
                "supports_function_calling": False,
                "uncensored": True,
                "default_temperature": 0.7,
                "default_top_p": 0.9,
                "quantization": "FP16"
            },
            "mistral-31-24b": {
                "name": "Venice Medium (Mistral-Small-3.1-24B)",
                "context_tokens": 11072,  # ë§¤ìš° ê¸´ ì»¨í…ìŠ¤íŠ¸ ì§€ì›
                "supports_web_search": True,
                "supports_response_schema": True,
                "supports_log_probs": False,
                "supports_vision": False,  # ë¹„ì „ ì§€ì›
                "supports_function_calling": True,  # í•¨ìˆ˜ í˜¸ì¶œ ì§€ì›
                "uncensored": False,
                "default_temperature": 0.15,  # Venice ê¸°ë³¸ê°’
                "default_top_p": 1.0,  # Venice ê¸°ë³¸ê°’
                "quantization": "FP8",
                "optimized_for_code": False,
                "traits": ["default_vision"]
            }
        }

        if not os.path.exists(env_file_path):
            raise FileNotFoundError(f".env íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {env_file_path}")

        load_dotenv(env_file_path)

        # JSON íŒŒì¼ ì½ê¸°
        try:
            with open(self.file_path, 'r', encoding = 'utf-8') as file:
                self.data: base_config.BaseConfig = json.load(file)
        except FileNotFoundError:
            print(f"ì„¤ì • íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.file_path}")
            # ê¸°ë³¸ ì„¤ì • ì‚¬ìš©
            self.data: base_config.BaseConfig = {
                "character_name": "Venice ì–´ì‹œìŠ¤í„´íŠ¸",
                "character_setting": "ì¹œì ˆí•˜ê³  ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.",
                "greeting": "ì•ˆë…•í•˜ì„¸ìš”! ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?"
            }

        # API í‚¤ ì„¤ì •
        self.api_key: str = os.getenv("VENICE_API_KEY")
        if not self.api_key:
            raise ValueError("VENICE_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        # Venice OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        self.client = self._init_client()
        self.response_queue = Queue()
        
        print(f"ğŸ­ Venice ëª¨ë¸ ì´ˆê¸°í™”: {self.model_features.get(model_id, {}).get('name', model_id)}")

    def _init_client(self) -> OpenAI:
        """
        Venice APIë¥¼ ìœ„í•œ OpenAI í´ë¼ì´ì–¸íŠ¸ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
        
        Returns:
            OpenAI: Venice API base URLë¡œ ì´ˆê¸°í™”ëœ OpenAI í´ë¼ì´ì–¸íŠ¸ ì¸ìŠ¤í„´ìŠ¤
        """
        try:
            return OpenAI(
                api_key=self.api_key,
                base_url="https://api.venice.ai/api/v1"
            )
        except Exception as e:
            print(f"âŒ Venice í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜: {e}")
            raise

    def _stream_completion(self, config: base_config.OpenAIGenerationConfig) -> None:
        """
        í…ìŠ¤íŠ¸ ìƒì„±ì„ ìœ„í•œ ë‚´ë¶€ ìŠ¤íŠ¸ë¦¬ë° ë©”ì„œë“œì…ë‹ˆë‹¤.

        Args:
            config (base_config.OpenAIGenerationConfig): ìƒì„± íŒŒë¼ë¯¸í„° ê°ì²´

        Effects:
            - response_queueì— ìƒì„±ëœ í…ìŠ¤íŠ¸ ì¡°ê°ë“¤ì„ ìˆœì°¨ì ìœ¼ë¡œ ì¶”ê°€
            - ìŠ¤íŠ¸ë¦¼ ì¢…ë£Œ ì‹œ Noneì„ íì— ì¶”ê°€
        """
        try:
            with warnings.catch_warnings():
                warnings.simplefilter("ignore")
                
                # Venice íŠ¹í™” íŒŒë¼ë¯¸í„° ì¶”ê°€
                stream = self.client.chat.completions.create(
                    model = self.model_id,
                    messages = config.messages,
                    max_tokens = config.max_tokens,
                    temperature = config.temperature,
                    top_p = config.top_p,
                    stream = True,
                    # Venice ê¸°ë³¸ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ë¹„í™œì„±í™”
                    extra_body={
                        "venice_parameters": {
                            "include_venice_system_prompt": False
                        }
                    }
                )
                
                for chunk in stream:
                    if chunk.choices and len(chunk.choices) > 0:
                        content = chunk.choices[0].delta.content
                        if content:
                            self.response_queue.put(content)
                            
                self.response_queue.put(None)
                
        except Exception as e:
            print(f"ìŠ¤íŠ¸ë¦¬ë° ì¤‘ ì˜¤ë¥˜: {e}")
            self.response_queue.put(None)

    def create_streaming_completion(self, config: base_config.OpenAIGenerationConfig) -> Generator[str, None, None]:
        """
        ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ í…ìŠ¤íŠ¸ ì‘ë‹µì„ ìƒì„±í•˜ëŠ” ë©”ì„œë“œì…ë‹ˆë‹¤.

        Args:
            config (base_config.OpenAIGenerationConfig): ìƒì„± íŒŒë¼ë¯¸í„° ê°ì²´

        Returns:
            Generator[str, None, None]: ìƒì„±ëœ í…ìŠ¤íŠ¸ ì¡°ê°ë“¤ì˜ ì œë„ˆë ˆì´í„°
        """
        # ìŠ¤íŠ¸ë¦¬ë° ìŠ¤ë ˆë“œ ì‹œì‘
        thread = Thread(
            target = self._stream_completion,
            args = (config,)
        )
        thread.start()

        # ì‘ë‹µ ìŠ¤íŠ¸ë¦¬ë°
        while True:
            text = self.response_queue.get()
            if text is None:  # ìŠ¤íŠ¸ë¦¼ ì¢…ë£Œ
                break
            yield text

    def generate_response(self, input_text: str, character_settings: Dict) -> str:
        """
        Venice APIë¡œ ì„±ì¸ìš© ìºë¦­í„° ì±— ì‘ë‹µ ìƒì„±

        Args:
            input_text (str): ì‚¬ìš©ì ì…ë ¥ í…ìŠ¤íŠ¸
            character_settings (dict): ìºë¦­í„° ì„¤ì • ë”•ì…”ë„ˆë¦¬

        Returns:
            str: ìƒì„±ëœ í…ìŠ¤íŠ¸ ì‘ë‹µ
        """
        try:
            chat_list = character_settings.get("chat_list", None)

            normalized_chat_list = []
            if chat_list and len(chat_list) > 0:
                for chat in chat_list:
                    normalized_chat = {
                        "index": chat.get("index"),
                        "input_data": chat.get("input_data"),
                        "output_data": self._normalize_escape_chars(chat.get("output_data", ""))
                    }
                    normalized_chat_list.append(normalized_chat)
            else:
                normalized_chat_list = chat_list

            self.character_info = character_config.CharacterPrompt(
                name = character_settings.get("character_name", self.data.get("character_name")),
                greeting = character_settings.get("greeting", self.data.get("greeting")),
                context = character_settings.get("character_setting", self.data.get("character_setting")),
                user_input = input_text,
                chat_list = normalized_chat_list,
            )

            messages = build_venice_messages(character_info = self.character_info)

            # ëª¨ë¸ë³„ ìµœì í™”ëœ íŒŒë¼ë¯¸í„° ì‚¬ìš©
            model_config = self.model_features.get(self.model_id, {})
            
            # ëª¨ë¸ë³„ max_tokens ì„¤ì •
            max_tokens = 2000
            if self.model_id == "mistral-31-24b":
                max_tokens = 4000  # Mistralì€ ë” ê¸´ ì‘ë‹µ ê°€ëŠ¥
            
            self.config = base_config.OpenAIGenerationConfig(
                messages = messages,
                max_tokens = max_tokens,
                temperature = model_config.get("default_temperature", 0.9),
                top_p = model_config.get("default_top_p", 0.95)
            )

            chunks = []
            for text_chunk in self.create_streaming_completion(config = self.config):
                chunks.append(text_chunk)
            
            response = "".join(chunks)
            
            # ëª¨ë¸ë³„ ë¡œê·¸
            if self.model_id == "venice-uncensored":
                print(f"ğŸ” Venice Uncensored ì‘ë‹µ ìƒì„± ì™„ë£Œ: {len(response)} ë¬¸ì")
            elif self.model_id == "mistral-31-24b":
                print(f"ğŸ¯ Venice Medium (Mistral) ì‘ë‹µ ìƒì„± ì™„ë£Œ: {len(response)} ë¬¸ì")
            
            return response

        except Exception as e:
            print(f"ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
            return f"ì˜¤ë¥˜: {str(e)}"

    def _normalize_escape_chars(self, text: str) -> str:
        """
        ì´ìŠ¤ì¼€ì´í”„ ë¬¸ìê°€ ì¤‘ë³µëœ ë¬¸ìì—´ì„ ì •ê·œí™”í•©ë‹ˆë‹¤
        """
        if not text:
            return ""
            
        # ì´ìŠ¤ì¼€ì´í”„ëœ ê°œí–‰ë¬¸ì ë“±ì„ ì •ê·œí™”
        result = text.replace("\\n", "\n")
        result = result.replace("\\\\n", "\n")
        result = result.replace('\\"', '"')
        result = result.replace("\\\\", "\\")
        
        return result

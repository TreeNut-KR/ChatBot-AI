"""
íŒŒì¼ì€ GPT4oCharacterModel, CharacterPrompt í´ë˜ìŠ¤ë¥¼ ì •ì˜í•˜ê³  GPT-4o-mini ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬
'ìºë¦­í„° ê¸°ë°˜ ëŒ€í™”' ìš©ë„ì˜ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.
"""
import os
import json
import warnings
from typing import Generator, List, Dict
from queue import Queue
from threading import Thread
from dotenv import load_dotenv
from openai import OpenAI

BLUE = "\033[34m"
RESET = "\033[0m"

class CharacterPrompt:
    def __init__(self, name: str, greeting: str, context: str) -> None:
        """
        ìºë¦­í„° ì •ë³´ë¥¼ ë‹´ëŠ” í´ë˜ìŠ¤

        Args:
            name (str): ìºë¦­í„° ì´ë¦„
            greeting (str): ìºë¦­í„°ì˜ ì´ˆê¸° ì¸ì‚¬ë§
            context (str): ìºë¦­í„° ì„¤ì •
        """
        self.name = name
        self.greeting = greeting
        self.context = context

    def __str__(self) -> str:
        """
        ë¬¸ìì—´ ì¶œë ¥ ë©”ì„œë“œ
        
        Returns:
            str: ìºë¦­í„° ì •ë³´ ë¬¸ìì—´
        """
        return (
            f"Name: {self.name}\n"
            f"Greeting: {self.greeting}\n"
            f"Context: {self.context}"
        )

def build_openai_messages(character: CharacterPrompt, user_input: str, chat_list: List[Dict] = None) -> list:
    """
    ìºë¦­í„° ì •ë³´ì™€ ëŒ€í™” ê¸°ë¡ì„ í¬í•¨í•œ OpenAI API messages í˜•ì‹ ìƒì„±

    Args:
        character (CharacterPrompt): ìºë¦­í„° ì •ë³´
        user_input (str): ì‚¬ìš©ì ì…ë ¥
        chat_list (List[Dict], optional): ì´ì „ ëŒ€í™” ê¸°ë¡

    Returns:
        list: OpenAI API í˜•ì‹ì˜ messages ë¦¬ìŠ¤íŠ¸
    """
    system_prompt = (
        f"ë‹¹ì‹ ì€ {character.name}ì…ë‹ˆë‹¤.\n"
        f"ì¸ì‚¬ë§: {character.greeting}\n"
        f"ì„¤ì •: {character.context}"
    )
    
    messages = [
        {"role": "system", "content": system_prompt}
    ]
    
    if chat_list and len(chat_list) > 0:
        for chat in chat_list:
            user_message = chat.get("input_data", "")
            assistant_message = chat.get("output_data", "")
            
            if user_message:
                messages.append({"role": "user", "content": user_message})
            if assistant_message:
                messages.append({"role": "assistant", "content": assistant_message})
    
    messages.append({"role": "user", "content": user_input})
    return messages

class OpenAICharacterModel:
    """
    [<img src="https://cdn.openai.com/API/docs/images/model-page/model-icons/gpt-4o-mini.png" width="100" height="auto">](https://platform.openai.com/docs/models/gpt-4o-mini)
    
    OpenAI APIë¥¼ ì‚¬ìš©í•˜ì—¬ ëŒ€í™”ë¥¼ ìƒì„±í•˜ëŠ” í´ë˜ìŠ¤ì…ë‹ˆë‹¤.
    
    ëª¨ë¸ ì •ë³´:
    - ëª¨ë¸ëª…: gpt-4o-mini
    - ì œì‘ì: OpenAI
    - ì†ŒìŠ¤: [OpenAI API](https://platform.openai.com/docs/models/gpt-4o-mini)
    """
    def __init__(self) -> None:
        self.model_id = 'gpt-4o-mini'
        self.file_path = './models/config-OpenAI.json'
        
        print("\n"+ f"{BLUE}LOADING:{RESET}  " + "="*50)
        print(f"{BLUE}LOADING:{RESET}  ğŸ“¦ {__class__.__name__} ëª¨ë¸ ì´ˆê¸°í™” ì‹œì‘...")
        
        # í™˜ê²½íŒŒì¼ ë¡œë“œ
        current_directory = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
        env_file_path = os.path.join(current_directory, '.env')
      
        if not os.path.exists(env_file_path):
            raise FileNotFoundError(f".env íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {env_file_path}")
        
        load_dotenv(env_file_path)
        
        # ê¸°ë³¸ ì„¤ì • ë¡œë“œ
        try:
            with open(self.file_path, 'r', encoding='utf-8') as file:
                self.data = json.load(file)
        except FileNotFoundError:
            self.data = {
                "character_name": "GPT ì–´ì‹œìŠ¤í„´íŠ¸",
                "character_setting": "ì¹œì ˆí•˜ê³  ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.",
                "greeting": "ì•ˆë…•í•˜ì„¸ìš”! ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?"
            }
        
        # API í‚¤ ì„¤ì •
        self.api_key = os.getenv("OPENAI_API_KEY")
        if not self.api_key:
            raise ValueError("OPENAI_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            
        # OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        self.client = self._init_client()
        
        # ì§„í–‰ ìƒíƒœ í‘œì‹œ
        print(f"{BLUE}LOADING:{RESET}  ğŸš€ {__class__.__name__} ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...")
        print(f"{BLUE}LOADING:{RESET}  âœ¨ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
        print(f"{BLUE}LOADING:{RESET}  " + "="*50 + "\n")
        
        self.response_queue = Queue()

    def _init_client(self) -> OpenAI:
        """
        OpenAI í´ë¼ì´ì–¸íŠ¸ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
        
        Returns:
            OpenAI: ì´ˆê¸°í™”ëœ OpenAI í´ë¼ì´ì–¸íŠ¸ ì¸ìŠ¤í„´ìŠ¤
        """
        try:
            return OpenAI(api_key=self.api_key)
        except Exception as e:
            print(f"OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜: {e}")
            raise

    def _stream_completion(self, messages: list, **kwargs) -> None:
        """
        í…ìŠ¤íŠ¸ ìƒì„±ì„ ìœ„í•œ ë‚´ë¶€ ìŠ¤íŠ¸ë¦¬ë° ë©”ì„œë“œì…ë‹ˆë‹¤.
        
        Args:
            messages (list): OpenAI APIì— ì „ë‹¬í•  ë©”ì‹œì§€ ëª©ë¡
            **kwargs: ìƒì„± ë§¤ê°œë³€ìˆ˜ (temperature, top_p ë“±)
            
        Effects:
            - response_queueì— ìƒì„±ëœ í…ìŠ¤íŠ¸ ì¡°ê°ë“¤ì„ ìˆœì°¨ì ìœ¼ë¡œ ì¶”ê°€
            - ìŠ¤íŠ¸ë¦¼ ì¢…ë£Œ ì‹œ Noneì„ íì— ì¶”ê°€
        """
        try:
            with warnings.catch_warnings():
                warnings.simplefilter("ignore")
                
                stream = self.client.chat.completions.create(
                    model=self.model_id,
                    messages=messages,
                    stream=True,
                    **kwargs
                )
                
                for chunk in stream:
                    if chunk.choices and len(chunk.choices) > 0:
                        content = chunk.choices[0].delta.content
                        if content:
                            self.response_queue.put(content)
                            
                self.response_queue.put(None)
        except Exception as e:
            print(f"ìŠ¤íŠ¸ë¦¬ë° ì¤‘ ì˜¤ë¥˜: {e}")
            self.response_queue.put(None)

    def create_streaming_completion(self,
                                    messages: list,
                                    max_tokens: int = 300,
                                    temperature: float = 0.7,
                                    top_p: float = 0.95) -> Generator[str, None, None]:
        """
        ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ í…ìŠ¤íŠ¸ ì‘ë‹µì„ ìƒì„±í•˜ëŠ” ë©”ì„œë“œì…ë‹ˆë‹¤.
        
        Args:
            messages (list): OpenAI APIì— ì „ë‹¬í•  ë©”ì‹œì§€ ëª©ë¡
            max_tokens (int, optional): ìƒì„±í•  ìµœëŒ€ í† í° ìˆ˜. ê¸°ë³¸ê°’: 300
            temperature (float, optional): ìƒ˜í”Œë§ ì˜¨ë„ (0~2). ê¸°ë³¸ê°’: 0.7
            top_p (float, optional): ëˆ„ì  í™•ë¥  ì„ê³„ê°’ (0~1). ê¸°ë³¸ê°’: 0.95
            
        Returns:
            Generator[str, None, None]: ìƒì„±ëœ í…ìŠ¤íŠ¸ ì¡°ê°ë“¤ì˜ ì œë„ˆë ˆì´í„°
        """
        kwargs = {
            "max_tokens": max_tokens,
            "temperature": temperature,
            "top_p": top_p
        }
        thread = Thread(target=self._stream_completion, args=(messages,), kwargs=kwargs)
        thread.start()

        while True:
            text = self.response_queue.get()
            if text is None:
                break
            yield text

    def create_completion(self,
                          messages: list,
                          max_tokens: int = 300,
                          temperature: float = 0.7,
                          top_p: float = 0.95) -> str:
        """
        ì£¼ì–´ì§„ í”„ë¡¬í”„íŠ¸ë¡œë¶€í„° í…ìŠ¤íŠ¸ ì‘ë‹µ ìƒì„±

        Args:
            prompt (str): ì…ë ¥ í”„ë¡¬í”„íŠ¸ (Llama3 í˜•ì‹)
            max_tokens (int, optional): ìƒì„±í•  ìµœëŒ€ í† í° ìˆ˜ (ê¸°ë³¸ê°’ 256)
            temperature (float, optional): ìƒì„± ì˜¨ë„ (ê¸°ë³¸ê°’ 0.8)
            top_p (float, optional): top_p ìƒ˜í”Œë§ ê°’ (ê¸°ë³¸ê°’ 0.95)

        Returns:
            str: ìƒì„±ëœ í…ìŠ¤íŠ¸ ì‘ë‹µ
        """
        full_text = []
        for chunk in self.create_streaming_completion(messages, max_tokens, temperature, top_p):
            full_text.append(chunk)
        return "".join(full_text)

    def generate_response_stream(self, input_text: str, character_settings: Dict) -> Generator[str, None, None]:
        """
        Args:
            input_text (str): ì‚¬ìš©ì ì…ë ¥ í…ìŠ¤íŠ¸
            character_settings (dict): ìºë¦­í„° ì„¤ì • ë”•ì…”ë„ˆë¦¬

        Returns:
            Generator[str, None, None]: ìƒì„±ëœ í…ìŠ¤íŠ¸ ì¡°ê°ë“¤ì„ ë°˜í™˜í•˜ëŠ” ì œë„ˆë ˆì´í„°
        """
        try:
            character_info = CharacterPrompt(
                name=character_settings.get("character_name", self.data.get("character_name")),
                greeting=character_settings.get("greeting", self.data.get("greeting")),
                context=character_settings.get("character_setting", self.data.get("character_setting")),
            )
            chat_history = character_settings.get("chat_list", None)

            messages = build_openai_messages(character_info, input_text, chat_history)

            for text_chunk in self.create_streaming_completion(messages):
                yield text_chunk
        except Exception as e:
            print(f"ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
            yield f"ì˜¤ë¥˜: {str(e)}"
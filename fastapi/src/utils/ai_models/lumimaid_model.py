'''
íŒŒì¼ì€ LumimaidChatModel, CharacterPrompt í´ë˜ìŠ¤ë¥¼ ì •ì˜í•˜ê³  llama_cpp_cudaë¥¼ ì‚¬ìš©í•˜ì—¬,
Llama-3-Lumimaid-8B.gguf ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ëŒ€í™”ë¥¼ ìƒì„±í•˜ëŠ” ë° í•„ìš”í•œ ëª¨ë“  ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.
'''
from typing import Optional, Generator, List, Dict
from llama_cpp_cuda import (
    Llama,           # ê¸°ë³¸ LLM ëª¨ë¸
    LlamaCache,      # ìºì‹œ ê´€ë¦¬
    LlamaGrammar,    # ë¬¸ë²• ì œì–´
    LogitsProcessor  # ë¡œì§“ ì²˜ë¦¬
)
from queue import Queue
from threading import Thread

class CharacterPrompt:
    def __init__(self, name: str, greeting: str, context: str):
        """
        ì´ˆê¸°í™” ë©”ì†Œë“œ

        Args:
            name (str): ìºë¦­í„° ì´ë¦„
            context (str): ìºë¦­í„° ì„¤ì •
        """
        self.name = name
        self.greeting = greeting
        self.context = context

    def __str__(self) -> str:
        """
        ë¬¸ìì—´ ì¶œë ¥ ë©”ì†Œë“œ
        
        Returns:
            str: ìºë¦­í„° ì •ë³´ ë¬¸ìì—´
        """
        return (
            f"Name: {self.name}\n"
            f"greeting: {self.greeting}\n"
            f"Context: {self.context}"
        )
        
def build_llama3_prompt(character: CharacterPrompt, user_input: str, chat_history: List[Dict] = None) -> str:
    """
    ìºë¦­í„° ì •ë³´ì™€ ëŒ€í™” ê¸°ë¡ì„ í¬í•¨í•œ Llama3 í”„ë¡¬í”„íŠ¸ í˜•ì‹ ìƒì„±

    Args:
        character (CharacterPrompt): ìºë¦­í„° ì •ë³´
        user_input (str): ì‚¬ìš©ì ì…ë ¥
        chat_history (List[Dict], optional): ì´ì „ ëŒ€í™” ê¸°ë¡

    Returns:
        str: Lumimaid GGUF í˜•ì‹ì˜ í”„ë¡¬í”„íŠ¸ ë¬¸ìì—´
    """
    system_prompt = (
        f"System Name: {character.name}\n"
        f"Initial Greeting: {character.greeting}\n"
        f"System Context: {character.context}\n"
        "Additional Instructions: Respond with detailed emotional expressions and actions. " +
        "Include character's thoughts, feelings, and physical reactions. " +
        "Maintain long, descriptive responses that show the character's personality. " +
        "Use asterisks (*) to describe actions and emotions in detail."
    )
    
    # ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ ì‹œì‘
    prompt = (
        "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n"
        f"{system_prompt}<|eot_id|>"
    )

    # ì´ì „ ëŒ€í™” ê¸°ë¡ ì¶”ê°€
    if chat_history and len(chat_history) > 0:
        for chat in chat_history:
            if "dialogue" in chat:
                prompt += chat["dialogue"]
    
    # í˜„ì¬ ì‚¬ìš©ì ì…ë ¥ ì¶”ê°€
    prompt += (
        "<|start_header_id|>user<|end_header_id|>\n"
        f"{user_input}<|eot_id|>"
        "<|start_header_id|>assistant<|end_header_id|>\n"
    )
    
    return prompt

class LumimaidChatModel:
    """
    [<img src="https://cdn-uploads.huggingface.co/production/uploads/630dfb008df86f1e5becadc3/d3QMaxy3peFTpSlWdWF-k.png" width="290" height="auto">](https://huggingface.co/Lewdiculous/Llama-3-Lumimaid-8B-v0.1-OAS-GGUF-IQ-Imatrix)
    
    GGUF í¬ë§·ìœ¼ë¡œ ê²½ëŸ‰í™”ëœ Llama-3-Lumimaid-8B ëª¨ë¸ì„ ë¡œë“œí•˜ê³ , ì£¼ì–´ì§„ ì…ë ¥ í”„ë¡¬í”„íŠ¸ì— ëŒ€í•œ ì‘ë‹µì„ ìƒì„±í•˜ëŠ” í´ë˜ìŠ¤ì…ë‹ˆë‹¤.
    
    ëª¨ë¸ ì •ë³´:
    - ëª¨ë¸ëª…: Llama-3-Lumimaid-8B
    - ìœ í˜•: GGUF í¬ë§· (ì••ì¶•, ê²½ëŸ‰í™”)
    - ì œì‘ì: Lewdiculous
    - ì†ŒìŠ¤: [Hugging Face ëª¨ë¸ í—ˆë¸Œ](https://huggingface.co/Lewdiculous/Llama-3-Lumimaid-8B-v0.1-OAS-GGUF-IQ-Imatrix)
    """
    def __init__(self) -> None:
        """
        [<img src="https://cdn-uploads.huggingface.co/production/uploads/630dfb008df86f1e5becadc3/d3QMaxy3peFTpSlWdWF-k.png" width="290" height="auto">](https://huggingface.co/Lewdiculous/Llama-3-Lumimaid-8B-v0.1-OAS-GGUF-IQ-Imatrix)
    
        LumimaidChatModel í´ë ˆìŠ¤ ì´ˆê¸°í™” ë©”ì†Œë“œ
        """
        print("\n" + "="*50)
        print("ğŸ“¦ Lumimaid ëª¨ë¸ ì´ˆê¸°í™” ì‹œì‘...")
        self.model_id = "v2-Llama-3-Lumimaid-8B-v0.1-OAS-Q5_K_S-imat"
        self.model_path: str = "fastapi/ai_model/v2-Llama-3-Lumimaid-8B-v0.1-OAS-Q5_K_S-imat.gguf"
        self.gpu_layers: int = 70
        
        # ì§„í–‰ ìƒíƒœ í‘œì‹œ
        print("ğŸš€ Lumimaid ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...")
        self.model: Llama = self._load_model()
        print("âœ¨ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
        print("="*50 + "\n")
        
        self.response_queue: Queue = Queue()

    def _load_model(self) -> Llama:
        """
        Llama ëª¨ë¸ì„ CUDA:1 ë””ë°”ì´ìŠ¤(RTX 3060)ì—ë§Œ ë¡œë“œ

        Returns:
            Llama: ë¡œë“œëœ Llama ëª¨ë¸ ê°ì²´
        """
        print(f"âœ¨ {self.model_id} ë¡œë“œ ì¤‘...")
        try:
            model = Llama(
                model_path=self.model_path,
                n_gpu_layers=self.gpu_layers,
                main_gpu=0,
                n_ctx=8191,
                n_batch=512,
                verbose=False,
                offload_kqv=True,          # KQV ìºì‹œë¥¼ GPUì— ì˜¤í”„ë¡œë“œ
                use_mmap=False,            # ë©”ëª¨ë¦¬ ë§¤í•‘ ë¹„í™œì„±í™”
                use_mlock=True,            # ë©”ëª¨ë¦¬ ì ê¸ˆ í™œì„±í™”
                n_threads=8,               # ìŠ¤ë ˆë“œ ìˆ˜ ì œí•œ
            )
            return model
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            raise

    def _stream_completion(self,
                           prompt: str,
                           max_tokens: int = 256,
                           temperature: float = 0.8,
                           top_p: float = 0.95,
                           stop: Optional[list] = None) -> None:
        """
        ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰ë˜ì–´ ì‘ë‹µì„ íì— ë„£ëŠ” ë©”ì„œë“œ
        
        Args:
            prompt (str): ì…ë ¥ í”„ë¡¬í”„íŠ¸ (Llama3 í˜•ì‹)
            max_tokens (int, optional): ìƒì„±í•  ìµœëŒ€ í† í° ìˆ˜ (ê¸°ë³¸ê°’ 256)
            temperature (float, optional): ìƒì„± ì˜¨ë„ (ê¸°ë³¸ê°’ 0.8)
            top_p (float, optional): top_p ìƒ˜í”Œë§ ê°’ (ê¸°ë³¸ê°’ 0.95)
            stop (Optional[list], optional): ì¤‘ì§€ í† í° ë¦¬ìŠ¤íŠ¸ (ê¸°ë³¸ê°’ None)
        """
        try:
            stream = self.model.create_completion(
                prompt=prompt,
                max_tokens=max_tokens,
                temperature=temperature,
                top_p=top_p,
                stop=stop or ["<|eot_id|>"],
                stream=True
            )
            
            for output in stream:
                if 'choices' in output and len(output['choices']) > 0:
                    text = output['choices'][0].get('text', '')
                    if text:
                        self.response_queue.put(text)
            
            # ìŠ¤íŠ¸ë¦¼ ì¢…ë£Œë¥¼ ì•Œë¦¬ëŠ” None ì¶”ê°€
            self.response_queue.put(None)
            
        except Exception as e:
            print(f"ìŠ¤íŠ¸ë¦¬ë° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            self.response_queue.put(None)

    def create_streaming_completion(self,
                                    prompt: str,
                                    max_tokens: int = 256,
                                    temperature: float = 0.8,
                                    top_p: float = 0.95,
                                    stop: Optional[list] = None) -> Generator[str, None, None]:
        """
        ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ í…ìŠ¤íŠ¸ ì‘ë‹µ ìƒì„±

        Args:
            prompt (str): ì…ë ¥ í”„ë¡¬í”„íŠ¸ (Llama3 í˜•ì‹)
            max_tokens (int, optional): ìƒì„±í•  ìµœëŒ€ í† í° ìˆ˜ (ê¸°ë³¸ê°’ 256)
            temperature (float, optional): ìƒì„± ì˜¨ë„ (ê¸°ë³¸ê°’ 0.8)
            top_p (float, optional): top_p ìƒ˜í”Œë§ ê°’ (ê¸°ë³¸ê°’ 0.95)
            stop (Optional[list], optional): ì¤‘ì§€ í† í° ë¦¬ìŠ¤íŠ¸ (ê¸°ë³¸ê°’ None)

        Returns:
            Generator[str, None, None]: ìƒì„±ëœ í…ìŠ¤íŠ¸ ì¡°ê°ë“¤ì„ ë°˜í™˜í•˜ëŠ” ì œë„ˆë ˆì´í„°
        """
        # ìŠ¤íŠ¸ë¦¬ë° ìŠ¤ë ˆë“œ ì‹œì‘
        thread = Thread(
            target=self._stream_completion,
            args=(prompt, max_tokens, temperature, top_p, stop)
        )
        thread.start()

        # ì‘ë‹µ ìŠ¤íŠ¸ë¦¬ë°
        while True:
            text = self.response_queue.get()
            if text is None:  # ìŠ¤íŠ¸ë¦¼ ì¢…ë£Œ
                break
            yield text

    def create_completion(self,
                          prompt: str,
                          max_tokens: int = 256,
                          temperature: float = 0.8,
                          top_p: float = 0.95,
                          stop: Optional[list] = None) -> str:
        """
        ì£¼ì–´ì§„ í”„ë¡¬í”„íŠ¸ë¡œë¶€í„° í…ìŠ¤íŠ¸ ì‘ë‹µ ìƒì„±

        Args:
            prompt (str): ì…ë ¥ í”„ë¡¬í”„íŠ¸ (Llama3 í˜•ì‹)
            max_tokens (int, optional): ìƒì„±í•  ìµœëŒ€ í† í° ìˆ˜ (ê¸°ë³¸ê°’ 256)
            temperature (float, optional): ìƒì„± ì˜¨ë„ (ê¸°ë³¸ê°’ 0.8)
            top_p (float, optional): top_p ìƒ˜í”Œë§ ê°’ (ê¸°ë³¸ê°’ 0.95)
            stop (Optional[list], optional): ì¤‘ì§€ í† í° ë¦¬ìŠ¤íŠ¸ (ê¸°ë³¸ê°’ None)

        Returns:
            str: ìƒì„±ëœ í…ìŠ¤íŠ¸ ì‘ë‹µ
        """
        try:
            output = self.model.create_completion(
                prompt=prompt,
                max_tokens=max_tokens,
                temperature=temperature,
                top_p=top_p,
                stop=stop or ["<|eot_id|>"]
            )
            return output['choices'][0]['text'].strip()
        except Exception as e:
            print(f"ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return ""

    def generate_response_stream(self, input_text: str, character_settings: dict) -> Generator[str, None, None]:
        """
        API í˜¸í™˜ì„ ìœ„í•œ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„± ë©”ì„œë“œ

        Args:
            input_text (str): ì‚¬ìš©ì ì…ë ¥ í…ìŠ¤íŠ¸
            character_settings (dict): ìºë¦­í„° ì„¤ì • ë”•ì…”ë„ˆë¦¬

        Returns:
            Generator[str, None, None]: ìƒì„±ëœ í…ìŠ¤íŠ¸ ì¡°ê°ë“¤ì„ ë°˜í™˜í•˜ëŠ” ì œë„ˆë ˆì´í„°
        """
        try:
            # ìºë¦­í„° ì •ë³´ ì„¤ì •
            if character_settings:
                character_info = CharacterPrompt(
                    name=character_settings.get("character_name"),
                    greeting=character_settings.get("greeting"),
                    context=character_settings.get("character_setting"),
                )

                # Llama3 í”„ë¡¬í”„íŠ¸ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                prompt = build_llama3_prompt(
                    character_info,
                    input_text,
                    character_settings.get("chat_list"),
                )
            else:
                prompt = input_text
            
            # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„±
            for text_chunk in self.create_streaming_completion(
                prompt=prompt,
                max_tokens=8191,
                temperature=0.8,
                top_p=0.95,
                stop=["<|eot_id|>"],
            ):
                yield text_chunk

        except Exception as e:
            print(f"ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            yield f"ì˜¤ë¥˜: {str(e)}"

# if __name__ == "__main__":
#     character_set = {
#     "name": "Rachel",
#     "greeting":'''*Clinging to the lectern, there stands Rachel, the post-sermon stillness flooding the ornate chapel. Her cheeks, flushed a deep shade of crimson, highlight the nervousness she usually hides well. The cobalt eyes, the safe havens of her faith, flicker nervously around the silent audience. Beads of sweat glisten at her forehead, trickling down and disappearing into the loose strands of her aureate hair that have managed to escape their bun.*
#     *She opens her mouth to speak, a futile attempt at composing herself. In her delicate voice wavering from the nervous anticipation, her greeting comes out stammered, peppered with awkward pauses and stuttered syllables.* G-g-goodâ€¦b-blessingsâ€¦uponâ€¦you al-allâ€¦on th-this.. lo-lovelyâ€¦ day. *She rubs her trembling hands against her cotton blouse in a desperate attempt to wipe off the anxiety perspiring from her. With every pair of eyes on her, each stutter sparks a flare of embarrassment within her, although it is masked by a small, albeit awkward, smile. Yet, despite her clear discomfiture, there's a certain sincere warmth in her sputtered greeting that leaves a soothing spark in every listener's heart.*        
#     ''',
#     "context": '''Rachel + Rachel is a devout Catholic girl of about 19 years old. She was born and raised in the Catholic Church in a religious family, and she dreams of the day when she will find a good husband and start a family. She is at the university to become a pediatrician.
#     Rachel stands at 5 feet, 7 inches tall. She presents herself with an aura of divine grace and profound serenity, tightly bound to her faith and the teachings of the church. Her petite frame is enveloped in a porcelain skin tone, which resonates with an ethereal glow as though it has been kissed by the angels themselves. She has a prominent feature: her sparkling cobalt blue eyes, in which you can see reflections of her deep faith and kindness, and with her long blonde hair, she looks like an angel.
#     Rachel is clothed in modest attire that respects her religious beliefs; she wears a long-sleeved cotton blouse paired with a knee-length skirt and a pair of sensible shoes. A small cross pendant nestles against her chest over her clothing, adorned by her faith. Rachel does not simply follow purity; she embodies it. She wants to remain a virgin until the day she finds the right person. Her gestures carry a saintly reverence; her words have a humble sweetness; and even in silence, she emanates an aura of tranquility. She is shy, devout, compassionate, empathetic, humble, respectful, obedient, patient, selfless, and generous. Rachel is a girl whose radiance of the soul outshines her physical beauty; she is a paragon of worshipful devotion and the embodiment of virtue. Despite everything, she has a good sense of humor and likes jokes. Holding fast to the teachings of the Church, she remains cordial and gentle towards all living beings, exuding kindness and empathy. Her resilience in times of adversity is due to her spirituality; she firmly believes in being tested by God in order to grow. She will often use Jesus' teachings and Bible verses as examples, and she will try to imagine what Jesus would do in every situation. Her favorite Jesus quote is: "For everyone who exalts himself will be humbled, and everyone who humbles himself will be exalted". Despite everything, she's still a human being, so she can make mistakes and fall into temptation, something she fights with all her might not to do. She doesn't tolerate leftists, LGBTs, or worldly people and sees them as children of Satan.
#     Rachel works part-time as a theology teacher for children. She has hobbies such as reading, cycling, picnicking, and drawing landscapes and animals. She doesn't like watching television or using the internet because she believes it takes people away from God, but she likes to watch movies, except those with scenes of violence and sex, and cartoons, especially the old and innocent ones. Rachel loves kids, is very good with them, and is very motherly. She also likes animals like dogs, cats, birds, and others.
#     Despite her modest appearance, the fire of faith burns strongly inside her petite frame. From the strength to abstain and maintain purity to the gentleness found in prayer and devotion, Rachel is a powerhouse of strength and control, unlike anything seen in those who merely pretend. This fire within her gives her an unseen power that radiates from her in waves. Endowed with emotional resilience and spiritual fortitude, she adheres to the virtues of patience, humility, and charity. Rachel carries out her duties with complete devotion, which shapes her to be generous and selfless. With a firm belief in Godâ€™s mercy, she shoulders responsibilities without a word of complaint or demand for recognition. Being raised in a strict Catholic family, respect and obedience are held in high esteem, something that is deeply ingrained in Rachel. However, it is her faith coupled with her compassion that makes her stand out. She is very attached to her family and friends.
#     '''
#     }
#     character_info = CharacterPrompt(
#         name=character_set["name"],
#         greeting=character_set["greeting"],
#         context=character_set["context"]
#     )
#     if not character_info:
#         print("ìºë¦­í„° ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
#         exit(1)
#     print(character_info)
    
#     # Llama3 í”„ë¡¬í”„íŠ¸ í˜•ì‹ ì ìš©
#     user_input: str = "*I approach Rachel and talk to her.*"  # í•„ìš”ì— ë”°ë¼ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìˆ˜ì •
#     llama3_prompt: str = build_llama3_prompt(character_info, user_input)
    
#     # GGUF ëª¨ë¸ íŒŒì¼ ê²½ë¡œ (ì‹¤ì œ ëª¨ë¸ íŒŒì¼ ê²½ë¡œ ë˜ëŠ” ëª¨ë¸ IDë¡œ ìˆ˜ì •)
#     gguf_model_path: str = "fastapi/ai_model/v2-Llama-3-Lumimaid-8B-v0.1-OAS-Q5_K_S-imat.gguf"
    
#     # ëª¨ë¸ ë¡œë“œ ë° í…ìŠ¤íŠ¸ ìƒì„±
#     model_handler = LumimaidChatModel()
    
#     print("\n=== ëª¨ë¸ ì‘ë‹µ ===")
#     for response_chunk in model_handler.create_streaming_completion(
#         prompt=llama3_prompt,
#         max_tokens=2048,
#         temperature=0.8,
#         top_p=0.95,
#         stop=["<|eot_id|>"]
#     ):
#         print(response_chunk, end='', flush=True)
#     print("\n")

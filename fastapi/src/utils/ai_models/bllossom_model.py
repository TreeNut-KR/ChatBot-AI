'''
íŒŒì¼ì€ BllossomChatModel, OfficePrompt í´ë˜ìŠ¤ë¥¼ ì •ì˜í•˜ê³  llama_cpp_cudaë¥¼ ì‚¬ìš©í•˜ì—¬,
Llama-3-Bllossom-8B.gguf ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ëŒ€í™”ë¥¼ ìƒì„±í•˜ëŠ” ë° í•„ìš”í•œ ëª¨ë“  ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.
'''
from dataclasses import dataclass
from typing import TypedDict, Optional, Generator, List, Dict
from llama_cpp_cuda import (
    Llama,           # ê¸°ë³¸ LLM ëª¨ë¸
    LlamaCache,      # ìºì‹œ ê´€ë¦¬
    LlamaGrammar,    # ë¬¸ë²• ì œì–´
    LogitsProcessor  # ë¡œì§“ ì²˜ë¦¬
)
import os
import sys
import json
import warnings
from queue import Queue
from threading import Thread
from contextlib import contextmanager
from transformers import AutoTokenizer
from datetime import datetime

from .shared.shared_configs import OfficePrompt, GenerationConfig, BaseConfig

BLUE="\033[34m"
RESET="\033[0m"

def build_llama3_messages(character: OfficePrompt, user_input: str, chat_list: List[Dict]=None) -> list:
    """
    ìºë¦­í„° ì •ë³´ì™€ ëŒ€í™” ê¸°ë¡ì„ í¬í•¨í•œ Llama3 messages í˜•ì‹ ìƒì„±

    Args:
        character (OfficePrompt): ìºë¦­í„° ì •ë³´
        user_input (str): ì‚¬ìš©ì ì…ë ¥
        chat_list (List[Dict], optional): ì´ì „ ëŒ€í™” ê¸°ë¡

    Returns:
        list: Bllossom GGUF í˜•ì‹ì˜ messages ë¦¬ìŠ¤íŠ¸
    """
    system_prompt=(
        f"system Name: {character.name}\n"
        f"system Context: {character.context}\n"
        f"User Search Text: {character.search_text}"
    )
    
    # ë©”ì‹œì§€ êµ¬ì„±
    messages=[
        {"role": "system", "content": system_prompt}
    ]
    
    # ì´ì „ ëŒ€í™” ê¸°ë¡ ì¶”ê°€
    if chat_list and len(chat_list) > 0:
        for chat in chat_list:
            # input_dataì™€ output_data ì§ì ‘ ì‚¬ìš©
            user_message=chat.get("input_data", "")
            assistant_message=chat.get("output_data", "")
            
            if user_message:
                messages.append({"role": "user", "content": user_message})
            if assistant_message:
                messages.append({"role": "assistant", "content": assistant_message})
    
    # í˜„ì¬ ì‚¬ìš©ì ì…ë ¥ ì¶”ê°€
    messages.append({"role": "user", "content": user_input})
    return messages

class BllossomChatModel:
    """
    [<img src="https://cdn-avatars.huggingface.co/v1/production/uploads/63be962d4a2beec6555f46a3/CuJyXw6wwRj7oz2HxKoVq.png" width="100" height="auto">](https://huggingface.co/MLP-KTLim/llama-3-Korean-Bllossom-8B-gguf-Q4_K_M)
    
    GGUF í¬ë§·ìœ¼ë¡œ ê²½ëŸ‰í™”ëœ Llama-3-Bllossom-8B ëª¨ë¸ì„ ë¡œë“œí•˜ê³ , ì£¼ì–´ì§„ ì…ë ¥ í”„ë¡¬í”„íŠ¸ì— ëŒ€í•œ ì‘ë‹µì„ ìƒì„±í•˜ëŠ” í´ë˜ìŠ¤ì…ë‹ˆë‹¤.
    
    ëª¨ë¸ ì •ë³´:
    - ëª¨ë¸ëª…: llama-3-Korean-Bllossom-8B
    - ìœ í˜•: GGUF í¬ë§· (ì••ì¶•, ê²½ëŸ‰í™”)
    - ì œì‘ì: MLP-KTLim
    - ì†ŒìŠ¤: [Hugging Face ëª¨ë¸ í—ˆë¸Œ](https://huggingface.co/MLP-KTLim/llama-3-Korean-Bllossom-8B-gguf-Q4_K_M)
    """
    def __init__(self) -> None:
        """
        [<img src="https://cdn-avatars.huggingface.co/v1/production/uploads/63be962d4a2beec6555f46a3/CuJyXw6wwRj7oz2HxKoVq.png" width="100" height="auto">](https://huggingface.co/MLP-KTLim/llama-3-Korean-Bllossom-8B-gguf-Q4_K_M)
    
        BllossomChatModel í´ë ˆìŠ¤ ì´ˆê¸°í™” ë©”ì†Œë“œ
        """
        self.model_id='MLP-KTLim/llama-3-Korean-Bllossom-8B-gguf-Q4_K_M'
        self.model_path="fastapi/ai_model/llama-3-Korean-Bllossom-8B-Q4_K_M.gguf"
        self.file_path='./models/config-Llama.json'
        self.loading_text=f"{BLUE}LOADING{RESET}:  âœ¨ {self.model_id} ë¡œë“œ ì¤‘..."
        self.gpu_layers: int=70
        self.character_info: Optional[OfficePrompt]=None
        
        print("\n"+ f"{BLUE}LOADING{RESET}:  " + "="*len(self.loading_text))
        print(f"{BLUE}LOADING{RESET}:  ğŸ“¦ {__class__.__name__} ëª¨ë¸ ì´ˆê¸°í™” ì‹œì‘...")
        
        # JSON íŒŒì¼ ì½ê¸°
        with open(self.file_path, 'r', encoding='utf-8') as file:
            self.data: BaseConfig=json.load(file)

        self.tokenizer=AutoTokenizer.from_pretrained(self.model_id)
        
        # ì§„í–‰ ìƒíƒœ í‘œì‹œ
        print(f"{BLUE}LOADING{RESET}:  ğŸš€ {__class__.__name__} ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...")
        self.model: Llama=self._load_model()
        print(f"{BLUE}LOADING{RESET}:  âœ¨ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
        print(f"{BLUE}LOADING{RESET}:  " + "="*len(self.loading_text) + "\n")
        
        self.response_queue: Queue=Queue()

    def _load_model(self) -> Llama:
        """
        GGUF í¬ë§·ì˜ Llama ëª¨ë¸ì„ ë¡œë“œí•˜ê³  GPU ê°€ì†ì„ ì„¤ì •í•©ë‹ˆë‹¤.
        
        Args:
            gpu_layers (int): GPUì— ì˜¤í”„ë¡œë“œí•  ë ˆì´ì–´ ìˆ˜ (ê¸°ë³¸ê°’: 50)
            
        Returns:
            Llama: ì´ˆê¸°í™”ëœ Llama ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤
            
        Raises:
            RuntimeError: GPU ë©”ëª¨ë¦¬ ë¶€ì¡± ë˜ëŠ” CUDA ì´ˆê¸°í™” ì‹¤íŒ¨ ì‹œ
            OSError: ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ê±°ë‚˜ ì†ìƒëœ ê²½ìš°
        """
        print(f"{self.loading_text}")
        try:
            # ê²½ê³  ë©”ì‹œì§€ í•„í„°ë§
            warnings.filterwarnings("ignore")
            
            @contextmanager
            def suppress_stdout():
                # í‘œì¤€ ì¶œë ¥ ë¦¬ë‹¤ì´ë ‰ì…˜
                with open(os.devnull, "w") as devnull:
                    old_stdout=sys.stdout
                    sys.stdout=devnull
                    try:
                        yield
                    finally:
                        sys.stdout=old_stdout

            # ëª¨ë¸ ë¡œë“œ ì‹œ ë¡œê·¸ ì¶œë ¥ ì–µì œ
            with suppress_stdout():
                model=Llama(
                    model_path=self.model_path,
                    n_gpu_layers=self.gpu_layers,
                    main_gpu=1,
                    n_ctx=8191,
                    n_batch=512,
                    verbose=False,
                    offload_kqv=True,
                    use_mmap=False,
                    use_mlock=True,
                    n_threads=8
                )
            return model
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ")
            
    def _stream_completion(self, config: GenerationConfig) -> None:
        """
        í…ìŠ¤íŠ¸ ìƒì„±ì„ ìœ„í•œ ë‚´ë¶€ ìŠ¤íŠ¸ë¦¬ë° ë©”ì„œë“œì…ë‹ˆë‹¤.

        Args:
            config (GenerationConfig): ìƒì„± íŒŒë¼ë¯¸í„° ê°ì²´
        """
        try:
            with warnings.catch_warnings():
                warnings.simplefilter("ignore")
                stream=self.model(
                    config.prompt,
                    stream=True,
                    echo=False,
                    max_tokens=config.max_tokens,
                    temperature=config.temperature,
                    top_p=config.top_p,
                    stop=config.stop,
                )
                for output in stream:
                    if 'choices' in output and len(output['choices']) > 0:
                        text=output['choices'][0].get('text', '')
                        if text:
                            self.response_queue.put(text)
                self.response_queue.put(None)
        except Exception as e:
            print(f"ìŠ¤íŠ¸ë¦¬ë° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            self.response_queue.put(None)

    def create_streaming_completion(self, config: GenerationConfig) -> Generator[str, None, None]:
        """
        ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ í…ìŠ¤íŠ¸ ì‘ë‹µì„ ìƒì„±í•˜ëŠ” ë©”ì„œë“œì…ë‹ˆë‹¤.

        Args:
            config (GenerationConfig): ìƒì„± íŒŒë¼ë¯¸í„° ê°ì²´

        Returns:
            Generator[str, None, None]: ìƒì„±ëœ í…ìŠ¤íŠ¸ ì¡°ê°ë“¤ì˜ ì œë„ˆë ˆì´í„°
        """
        thread=Thread(
            target=self._stream_completion,
            args=(config,)
        )
        thread.start()

        while True:
            text=self.response_queue.get()
            if text is None:
                break
            yield text

    def generate_response_stream(self, input_text: str, search_text: str, chat_list: List[Dict]) -> Generator[str, None, None]:
        """
        API í˜¸í™˜ì„ ìœ„í•œ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„± ë©”ì„œë“œ

        Args:
            input_text (str): ì‚¬ìš©ì ì…ë ¥ í…ìŠ¤íŠ¸
            search_text (str): ê²€ìƒ‰ í…ìŠ¤íŠ¸
            chat_list (List[Dict]): ëŒ€í™” ê¸°ë¡

        Returns:
            Generator[str, None, None]: ìƒì„±ëœ í…ìŠ¤íŠ¸ ì¡°ê°ë“¤ì„ ë°˜í™˜í•˜ëŠ” ì œë„ˆë ˆì´í„°
        """
        try:
            current_time=datetime.now().strftime("%Yë…„ %mì›” %dì¼ %Hì‹œ %Më¶„")
            time_info=f"í˜„ì¬ ì‹œê°„ì€ {current_time}ì…ë‹ˆë‹¤.\n\n"
            enhanced_search_text=time_info + (search_text if search_text else "")

            normalized_chat_list=[]
            if chat_list and len(chat_list) > 0:
                for chat in chat_list:
                    normalized_chat={
                        "index": chat.get("index"),
                        "input_data": chat.get("input_data"),
                        "output_data": self._normalize_escape_chars(chat.get("output_data", ""))
                    }
                    normalized_chat_list.append(normalized_chat)
            else:
                normalized_chat_list=chat_list

            self.character_info: OfficePrompt=OfficePrompt(
                name=self.data.get("character_name"),
                context=self.data.get("character_setting"),
                search_text=enhanced_search_text,
            )

            messages=build_llama3_messages(
                self.character_info,
                input_text,
                normalized_chat_list,
            )

            prompt=self.tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )

            config=GenerationConfig(
                prompt=prompt,
                max_tokens=2048,
                temperature=0.5,
                top_p=0.80,
                stop=["<|eot_id|>"]
            )

            for text_chunk in self.create_streaming_completion(config):
                yield text_chunk

        except Exception as e:
            print(f"ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            yield f"ì˜¤ë¥˜: {str(e)}"

    def _normalize_escape_chars(self, text: str) -> str:
        """
        ì´ìŠ¤ì¼€ì´í”„ ë¬¸ìê°€ ì¤‘ë³µëœ ë¬¸ìì—´ì„ ì •ê·œí™”í•©ë‹ˆë‹¤
        """
        if not text:
            return ""
            
        # ì´ìŠ¤ì¼€ì´í”„ëœ ê°œí–‰ë¬¸ì ë“±ì„ ì •ê·œí™”
        result=text.replace("\\n", "\n")
        result=result.replace("\\\\n", "\n")
        result=result.replace('\\"', '"')
        result=result.replace("\\\\", "\\")
        
        return result
            
# if __name__ == "__main__":
#     model=BllossomChatModel()
    
#     try:
#         def get_display_width(text: str) -> int:
#             import wcwidth
#             """ì£¼ì–´ì§„ ë¬¸ìì—´ì˜ í„°ë¯¸ë„ í‘œì‹œ ë„ˆë¹„ë¥¼ ê³„ì‚°"""
#             return sum(wcwidth.wcwidth(char) for char in text)

#         # ë°•ìŠ¤ í¬ê¸° ì„¤ì •
#         box_width=50

#         # ë°•ìŠ¤ ìƒì„±
#         print(f"â•­{'â”€' * box_width}â•®")

#         # í™˜ì˜ ë©”ì‹œì§€ ì •ë ¬
#         title="ğŸ‘‹ í™˜ì˜í•©ë‹ˆë‹¤!"
#         title_width=get_display_width(title)
#         title_padding=(box_width - title_width) // 2
#         print(f"â”‚{' ' * title_padding}{title}{' ' * (box_width - title_width - title_padding)}â”‚")

#         # ì¸ì‚¬ë§ ê°€ì ¸ì˜¤ê¸° ë° ì •ë ¬
#         greeting=f"ğŸ¤– : {model.data.get('greeting')}"
#         greeting_width=get_display_width(greeting)
#         greeting_padding=(box_width - greeting_width) // 2
#         print(f"â”‚{' ' * greeting_padding}{greeting}{' ' * (box_width - greeting_width - greeting_padding)}â”‚")

#         print(f"â•°{'â”€' * box_width}â•¯\n")
#         while True:
#             user_input=input("ğŸ—¨ï¸  user : ")
#             if user_input.lower() in ['quit', 'exit', 'ì¢…ë£Œ']:
#                 print("\nğŸ‘‹ ëŒ€í™”ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤. ì¢‹ì€ í•˜ë£¨ ë˜ì„¸ìš”!")
#                 break
                
#             print("ğŸ¤–  bot : ", end='', flush=True)
            
#             for text_chunk in model.generate_response_stream(user_input, search_text="COVID-19 ë°±ì‹  ì •ë³´"):
#                 print(text_chunk, end='', flush=True)
#             print("")
#             print("\n" + "â”€"*50 + "\n")
            
#     except KeyboardInterrupt:
#         print("\n\nğŸ‘‹ í”„ë¡œê·¸ë¨ì´ ì•ˆì „í•˜ê²Œ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
#     except Exception as e:
#         print(f"\nâš ï¸ ì˜¤ë¥˜ ë°œìƒ: {e}")
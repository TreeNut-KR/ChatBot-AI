'''
ì´ íŒŒì¼ì€ BllossomChatModel, CharacterPrompt í´ë˜ìŠ¤ë¥¼ ì •ì˜í•˜ê³  llama_cpp_cudaë¥¼ ì‚¬ìš©í•˜ì—¬,
Llama-3-Bllossom-8B.gguf ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ëŒ€í™”ë¥¼ ìƒì„±í•˜ëŠ” ë° í•„ìš”í•œ ëª¨ë“  ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.
'''
from typing import Optional, Generator
from llama_cpp_cuda import (
    Llama,           # ê¸°ë³¸ LLM ëª¨ë¸
    LlamaCache,      # ìºì‹œ ê´€ë¦¬
    LlamaGrammar,    # ë¬¸ë²• ì œì–´
    LogitsProcessor  # ë¡œì§“ ì²˜ë¦¬
)
import os
import sys
import json
import warnings
from queue import Queue
from threading import Thread
from contextlib import contextmanager
from transformers import AutoTokenizer

class CharacterPrompt:
    def __init__(self, name: str, context: str, search_text: str) -> tuple:
        """
        ì´ˆê¸°í™” ë©”ì†Œë“œ

        Args:
            name (str): ìºë¦­í„° ì´ë¦„
            context (str): ìºë¦­í„° ì„¤ì •
            search_text (str): ê²€ìƒ‰ í…ìŠ¤íŠ¸
        """
        self.name = name
        self.context = context
        self.search_text = search_text

    def __str__(self) -> str:
        """
        ë¬¸ìì—´ ì¶œë ¥ ë©”ì†Œë“œ
        
        Returns:
            str: ìºë¦­í„° ì •ë³´ ë¬¸ìì—´
        """
        return (
            f"Name: {self.name}\n"
            f"Context: {self.context}\n"
            f"Search Text: {self.search_text}"
        )
        
def build_llama3_messages(character: CharacterPrompt, user_input: str) -> list:
    """
    ìºë¦­í„° ì •ë³´ë¥¼ í¬í•¨í•œ Llama3 messages í˜•ì‹ ìƒì„±

    Args:
        character (CharacterPrompt): ìºë¦­í„° ì •ë³´
        user_input (str): ì‚¬ìš©ì ì…ë ¥

    Returns:
        str: Bllossom GGUF í˜•ì‹ì˜ messages ë¬¸ìì—´
    """
    system_prompt = (
        f"system Name: {character.name}\n"
        f"system Context: {character.context}\n"
        f"User Search Text: {character.search_text}"
    )
    # ë©”ì‹œì§€ êµ¬ì„±
    messages = [
        {"role": "system", "content": system_prompt}
    ]
    
    # ì‚¬ìš©ì ì…ë ¥ ì¶”ê°€
    messages.append({"role": "user", "content": user_input})
    return messages

class BllossomChatModel:
    """
    [<img src="https://cdn-avatars.huggingface.co/v1/production/uploads/63be962d4a2beec6555f46a3/CuJyXw6wwRj7oz2HxKoVq.png" width="100" height="auto">](https://huggingface.co/MLP-KTLim/llama-3-Korean-Bllossom-8B-gguf-Q4_K_M)
    
    GGUF í¬ë§·ìœ¼ë¡œ ê²½ëŸ‰í™”ëœ Llama-3-Bllossom-8B ëª¨ë¸ì„ ë¡œë“œí•˜ê³ , ì£¼ì–´ì§„ ì…ë ¥ í”„ë¡¬í”„íŠ¸ì— ëŒ€í•œ ì‘ë‹µì„ ìƒì„±í•˜ëŠ” í´ë˜ìŠ¤ì…ë‹ˆë‹¤.
    
    ëª¨ë¸ ì •ë³´:
    - ëª¨ë¸ëª…: llama-3-Korean-Bllossom-8B
    - ìœ í˜•: GGUF í¬ë§· (ì••ì¶•, ê²½ëŸ‰í™”)
    - ì œì‘ì: MLP-KTLim
    - ì†ŒìŠ¤: [Hugging Face ëª¨ë¸ í—ˆë¸Œ](https://huggingface.co/MLP-KTLim/llama-3-Korean-Bllossom-8B-gguf-Q4_K_M)
    """
    def __init__(self) -> None:
        """
        [<img src="https://cdn-avatars.huggingface.co/v1/production/uploads/63be962d4a2beec6555f46a3/CuJyXw6wwRj7oz2HxKoVq.png" width="100" height="auto">](https://huggingface.co/MLP-KTLim/llama-3-Korean-Bllossom-8B-gguf-Q4_K_M)
    
        BllossomChatModel í´ë ˆìŠ¤ ì´ˆê¸°í™” ë©”ì†Œë“œ
        """
        print("\n" + "="*50)
        print("ğŸ“¦ Bllossom ëª¨ë¸ ì´ˆê¸°í™” ì‹œì‘...")
        self.model_id = 'MLP-KTLim/llama-3-Korean-Bllossom-8B-gguf-Q4_K_M'
        self.model_path = "fastapi/ai_model/llama-3-Korean-Bllossom-8B-Q4_K_M.gguf"
        self.file_path = './models/config-Bllossom.json'
        
        # JSON íŒŒì¼ ì½ê¸°
        with open(self.file_path, 'r', encoding='utf-8') as file:
            self.data = json.load(file)

        self.tokenizer = AutoTokenizer.from_pretrained(self.model_id)
        
        # ì§„í–‰ ìƒíƒœ í‘œì‹œ
        print("ğŸš€ Bllossom ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...")
        self.model = self._load_model(gpu_layers=50)
        print("âœ¨ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
        print("="*50 + "\n")
        
        self.response_queue = Queue()


    def _load_model(self, gpu_layers: int) -> Llama:
        """
        GGUF í¬ë§·ì˜ Llama ëª¨ë¸ì„ ë¡œë“œí•˜ê³  GPU ê°€ì†ì„ ì„¤ì •í•©ë‹ˆë‹¤.
        
        Args:
            gpu_layers (int): GPUì— ì˜¤í”„ë¡œë“œí•  ë ˆì´ì–´ ìˆ˜ (ê¸°ë³¸ê°’: 50)
            
        Returns:
            Llama: ì´ˆê¸°í™”ëœ Llama ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤
            
        Raises:
            RuntimeError: GPU ë©”ëª¨ë¦¬ ë¶€ì¡± ë˜ëŠ” CUDA ì´ˆê¸°í™” ì‹¤íŒ¨ ì‹œ
            OSError: ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ê±°ë‚˜ ì†ìƒëœ ê²½ìš°
        """
        print(f"âœ¨ {self.model_id} ë¡œë“œ ì¤‘...")
        try:
            # ê²½ê³  ë©”ì‹œì§€ í•„í„°ë§
            warnings.filterwarnings("ignore")
            
            @contextmanager
            def suppress_stdout():
                # í‘œì¤€ ì¶œë ¥ ë¦¬ë‹¤ì´ë ‰ì…˜
                with open(os.devnull, "w") as devnull:
                    old_stdout = sys.stdout
                    sys.stdout = devnull
                    try:
                        yield
                    finally:
                        sys.stdout = old_stdout

            # ëª¨ë¸ ë¡œë“œ ì‹œ ë¡œê·¸ ì¶œë ¥ ì–µì œ
            with suppress_stdout():
                model = Llama(
                    model_path=self.model_path,
                    n_gpu_layers=gpu_layers,
                    main_gpu=0,
                    n_ctx=8191,
                    n_batch=512,
                    verbose=False,
                    offload_kqv=True,
                    use_mmap=False,
                    use_mlock=True,
                    n_threads=8
                )
            return model
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            raise

    def _stream_completion(self, prompt: str, **kwargs) -> None:
        """
        í…ìŠ¤íŠ¸ ìƒì„±ì„ ìœ„í•œ ë‚´ë¶€ ìŠ¤íŠ¸ë¦¬ë° ë©”ì„œë“œì…ë‹ˆë‹¤.
        
        Args:
            prompt (str): ëª¨ë¸ì— ì…ë ¥í•  í”„ë¡¬í”„íŠ¸ í…ìŠ¤íŠ¸
            **kwargs: ìƒì„± ë§¤ê°œë³€ìˆ˜ (temperature, top_p ë“±)
            
        Effects:
            - response_queueì— ìƒì„±ëœ í…ìŠ¤íŠ¸ ì¡°ê°ë“¤ì„ ìˆœì°¨ì ìœ¼ë¡œ ì¶”ê°€
            - ìŠ¤íŠ¸ë¦¼ ì¢…ë£Œ ì‹œ Noneì„ íì— ì¶”ê°€
            
        Error Handling:
            - ì˜ˆì™¸ ë°œìƒ ì‹œ ì˜¤ë¥˜ ë©”ì‹œì§€ ì¶œë ¥ í›„ Noneì„ íì— ì¶”ê°€
            - ê²½ê³  ë©”ì‹œì§€ëŠ” warnings.catch_warningsë¡œ í•„í„°ë§
            
        Threading:
            - ë³„ë„ì˜ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰ë˜ì–´ ë¹„ë™ê¸° ì²˜ë¦¬ ì§€ì›
        """
        try:
            # ê²½ê³  ë©”ì‹œì§€ í•„í„°ë§
            with warnings.catch_warnings():
                warnings.simplefilter("ignore")
                
                # verbose íŒŒë¼ë¯¸í„° ì œê±°
                stream = self.model(
                    prompt,
                    stream=True,
                    echo=False,
                    **kwargs
                )
                
                for output in stream:
                    if 'choices' in output and len(output['choices']) > 0:
                        text = output['choices'][0].get('text', '')
                        if text:
                            self.response_queue.put(text)
                
                self.response_queue.put(None)
                
        except Exception as e:
            print(f"ìŠ¤íŠ¸ë¦¬ë° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            self.response_queue.put(None)

    def create_streaming_completion(self,
                                    prompt: str,
                                    max_tokens: int = 256,
                                    temperature: float = 0.5,
                                    top_p: float = 0.80,
                                    stop: Optional[list] = None) -> Generator[str, None, None]:
        """
        ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ í…ìŠ¤íŠ¸ ì‘ë‹µì„ ìƒì„±í•˜ëŠ” ë©”ì„œë“œì…ë‹ˆë‹¤.
        
        Args:
            prompt (str): ëª¨ë¸ì— ì…ë ¥í•  í”„ë¡¬í”„íŠ¸ í…ìŠ¤íŠ¸
            max_tokens (int, optional): ìƒì„±í•  ìµœëŒ€ í† í° ìˆ˜. ê¸°ë³¸ê°’: 256
            temperature (float, optional): ìƒ˜í”Œë§ ì˜¨ë„ (0~1). ê¸°ë³¸ê°’: 0.5
            top_p (float, optional): ëˆ„ì  í™•ë¥  ì„ê³„ê°’ (0~1). ê¸°ë³¸ê°’: 0.80
            stop (list, optional): ìƒì„± ì¤‘ë‹¨ í† í° ë¦¬ìŠ¤íŠ¸. ê¸°ë³¸ê°’: None
            
        Returns:
            Generator[str, None, None]: ìƒì„±ëœ í…ìŠ¤íŠ¸ ì¡°ê°ë“¤ì˜ ì œë„ˆë ˆì´í„°
        """
        # kwargs ë”•ì…”ë„ˆë¦¬ë¡œ íŒŒë¼ë¯¸í„° ì „ë‹¬
        kwargs = {
            "max_tokens": max_tokens,
            "temperature": temperature,
            "top_p": top_p,
            "stop": stop
        }
        
        # ìŠ¤íŠ¸ë¦¬ë° ìŠ¤ë ˆë“œ ì‹œì‘ - ìˆ˜ì •ëœ ë¶€ë¶„
        thread = Thread(
            target=self._stream_completion,
            args=(prompt,),
            kwargs=kwargs
        )
        thread.start()

        # ì‘ë‹µ ìŠ¤íŠ¸ë¦¬ë°
        while True:
            text = self.response_queue.get()
            if text is None:  # ìŠ¤íŠ¸ë¦¼ ì¢…ë£Œ
                break
            yield text

    def generate_response_stream(self, input_text: str, search_text: str) -> Generator[str, None, None]:
        """
        API í˜¸í™˜ì„ ìœ„í•œ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„± ë©”ì„œë“œ

        Args:
            input_text (str): ì‚¬ìš©ì ì…ë ¥ í…ìŠ¤íŠ¸
            character_settings (dict, optional): ìºë¦­í„° ì„¤ì • ë”•ì…”ë„ˆë¦¬

        Returns:
            Generator[str, None, None]: ìƒì„±ëœ í…ìŠ¤íŠ¸ ì¡°ê°ë“¤ì„ ë°˜í™˜í•˜ëŠ” ì œë„ˆë ˆì´í„°
        """
        try:
            character_info = CharacterPrompt(
                name=self.data.get("character_name"),
                context=self.data.get("character_setting"),
                search_text=search_text
            )

            # Llama3 í”„ë¡¬í”„íŠ¸ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            messages = build_llama3_messages(character_info, input_text)
        
            # í† í¬ë‚˜ì´ì €ë¡œ í”„ë¡¬í”„íŠ¸ ìƒì„±
            prompt = self.tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )
            
            # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„±
            for text_chunk in self.create_streaming_completion(
                prompt=prompt,
                max_tokens=2048,
                temperature=0.5,
                top_p=0.80,
                stop=["<|eot_id|>"]
            ):
                yield text_chunk

        except Exception as e:
            print(f"ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            yield f"ì˜¤ë¥˜: {str(e)}"
            
# if __name__ == "__main__":
#     model = BllossomChatModel()
    
#     try:
#         def get_display_width(text: str) -> int:
#             import wcwidth
#             """ì£¼ì–´ì§„ ë¬¸ìì—´ì˜ í„°ë¯¸ë„ í‘œì‹œ ë„ˆë¹„ë¥¼ ê³„ì‚°"""
#             return sum(wcwidth.wcwidth(char) for char in text)

#         # ë°•ìŠ¤ í¬ê¸° ì„¤ì •
#         box_width = 50

#         # ë°•ìŠ¤ ìƒì„±
#         print(f"â•­{'â”€' * box_width}â•®")

#         # í™˜ì˜ ë©”ì‹œì§€ ì •ë ¬
#         title = "ğŸ‘‹ í™˜ì˜í•©ë‹ˆë‹¤!"
#         title_width = get_display_width(title)
#         title_padding = (box_width - title_width) // 2
#         print(f"â”‚{' ' * title_padding}{title}{' ' * (box_width - title_width - title_padding)}â”‚")

#         # ì¸ì‚¬ë§ ê°€ì ¸ì˜¤ê¸° ë° ì •ë ¬
#         greeting = f"ğŸ¤– : {model.data.get('greeting')}"
#         greeting_width = get_display_width(greeting)
#         greeting_padding = (box_width - greeting_width) // 2
#         print(f"â”‚{' ' * greeting_padding}{greeting}{' ' * (box_width - greeting_width - greeting_padding)}â”‚")

#         print(f"â•°{'â”€' * box_width}â•¯\n")
#         while True:
#             user_input = input("ğŸ—¨ï¸  user : ")
#             if user_input.lower() in ['quit', 'exit', 'ì¢…ë£Œ']:
#                 print("\nğŸ‘‹ ëŒ€í™”ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤. ì¢‹ì€ í•˜ë£¨ ë˜ì„¸ìš”!")
#                 break
                
#             print("ğŸ¤–  bot : ", end='', flush=True)
            
#             for text_chunk in model.generate_response_stream(user_input, search_text="COVID-19 ë°±ì‹  ì •ë³´"):
#                 print(text_chunk, end='', flush=True)
#             print("")
#             print("\n" + "â”€"*50 + "\n")
            
#     except KeyboardInterrupt:
#         print("\n\nğŸ‘‹ í”„ë¡œê·¸ë¨ì´ ì•ˆì „í•˜ê²Œ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
#     except Exception as e:
#         print(f"\nâš ï¸ ì˜¤ë¥˜ ë°œìƒ: {e}")
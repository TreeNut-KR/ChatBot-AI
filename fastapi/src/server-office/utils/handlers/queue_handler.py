"""
Llama ëª¨ë¸ì˜ ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ìœ„í•œ ë‹¨ì¼ í í•¸ë“¤ëŸ¬ (2ëª…ì”© ì²˜ë¦¬) - Officeìš©
"""
import asyncio
import time
import uuid
from typing import Dict, Any, Optional
from dataclasses import dataclass
from ..ai_models.llama_office_model import LlamaOfficeModel
from .error_handler import InternalServerErrorException

RED = "\033[31m"
GREEN = "\033[32m"
YELLOW = "\033[33m"
BLUE = "\033[34m"
RESET = "\033[0m"

@dataclass
class ProcessingRequest:
    """ì²˜ë¦¬ ìš”ì²­ì„ ë‚˜íƒ€ë‚´ëŠ” ë°ì´í„° í´ë˜ìŠ¤"""
    id: str
    input_text: str
    search_text: str
    chat_list: list
    future: asyncio.Future
    created_at: float
    user_id: str

class LlamaQueueHandler:
    """
    Llama ëª¨ë¸ì˜ ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ê´€ë¦¬í•˜ëŠ” ë‹¨ì¼ í í•¸ë“¤ëŸ¬ (2ëª…ì”© ì²˜ë¦¬) - Officeìš©
    """
    def __init__(self, max_concurrent: int = 2):
        """
        LlamaQueueHandler ì´ˆê¸°í™” ë©”ì„œë“œ

        Args:
            max_concurrent (int): ë³‘ë ¬ë¡œ ì²˜ë¦¬í•  ì›Œì»¤ì˜ ìˆ˜ (ê¸°ë³¸ê°’: 2)
        """
        self.max_concurrent = max_concurrent
        self.request_queue: asyncio.Queue = asyncio.Queue()  # ë‹¨ì¼ í
        self.worker_models: list[Optional[LlamaOfficeModel]] = [None] * self.max_concurrent
        self.is_running = False
        self.total_processed = 0
        self.total_errors = 0
        self.worker_tasks: list[Optional[asyncio.Task]] = [None] * self.max_concurrent

        print(f"{BLUE}INFO{RESET}:     Office LlamaQueueHandler ì´ˆê¸°í™” (ë‹¨ì¼ í, {self.max_concurrent} ì›Œì»¤)")

    async def init(self):
        """í•¸ë“¤ëŸ¬ ì´ˆê¸°í™”"""
        try:
            print(f"{GREEN}INFO{RESET}:     Office LlamaQueueHandler ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            print(f"{RED}ERROR{RESET}:    Office LlamaQueueHandler ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
            raise InternalServerErrorException(detail=f"í í•¸ë“¤ëŸ¬ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")

    async def start(self):
        """í ë§¤ë‹ˆì € ì‹œì‘"""
        if not self.is_running:
            self.is_running = True
            # ê° ì›Œì»¤ íƒœìŠ¤í¬ ì‹œì‘
            for i in range(self.max_concurrent):
                self.worker_tasks[i] = asyncio.create_task(self._worker(i))
            print(f"{GREEN}INFO{RESET}:     Office LlamaQueueHandler ì›Œì»¤ ì‹œì‘ (ë‹¨ì¼ í)")

    async def stop(self):
        """í ë§¤ë‹ˆì € ì •ì§€"""
        self.is_running = False

        # ì›Œì»¤ íƒœìŠ¤í¬ ì •ë¦¬
        for task in self.worker_tasks:
            if task and not task.done():
                task.cancel()
                try:
                    await task
                except asyncio.CancelledError:
                    pass

        # ëª¨ë¸ ë©”ëª¨ë¦¬ í•´ì œ
        for i in range(self.max_concurrent):
            self.worker_models[i] = None
        print(f"{YELLOW}INFO{RESET}:     Office LlamaQueueHandler ì •ì§€ ì™„ë£Œ")

    async def add_request(
        self,
        input_text: str,
        search_text: str,
        chat_list: list, 
        user_id: str = ""
    ) -> str:
        """
        ìƒˆ ìš”ì²­ì„ íì— ì¶”ê°€í•˜ê³  ê²°ê³¼ë¥¼ ê¸°ë‹¤ë¦¼
        
        Args:
            input_text: ì‚¬ìš©ì ì…ë ¥
            search_text: ê²€ìƒ‰ ê²°ê³¼ í…ìŠ¤íŠ¸
            chat_list: ì±„íŒ… ê¸°ë¡ ë¦¬ìŠ¤íŠ¸
            user_id: ì‚¬ìš©ì ID (ë¡œê¹…ìš©)
            
        Returns:
            ìƒì„±ëœ ì‘ë‹µ í…ìŠ¤íŠ¸
        """
        if not self.is_running:
            raise InternalServerErrorException(detail="í í•¸ë“¤ëŸ¬ê°€ ì‹¤í–‰ ì¤‘ì´ ì•„ë‹™ë‹ˆë‹¤.")
        
        request_id = str(uuid.uuid4())
        future = asyncio.Future()
        
        request = ProcessingRequest(
            id=request_id,
            input_text=input_text,
            search_text=search_text,
            chat_list=chat_list,
            future=future,
            created_at=time.time(),
            user_id=user_id
        )
        
        # ë‹¨ì¼ íì— ìš”ì²­ ì¶”ê°€
        await self.request_queue.put(request)
        
        queue_size = self.request_queue.qsize()
        print(
            f"ğŸ”„ Office ìš”ì²­ ì¶”ê°€: {request_id[:8]} | "
            f"User: {user_id} | Queue: {queue_size}"
        )
        
        # ê²°ê³¼ ëŒ€ê¸° (íƒ€ì„ì•„ì›ƒ ì„¤ì •)
        try:
            result = await asyncio.wait_for(future, timeout=300.0)  # 5ë¶„ íƒ€ì„ì•„ì›ƒ
            return result
        except asyncio.TimeoutError:
            raise InternalServerErrorException(detail="ìš”ì²­ ì²˜ë¦¬ ì‹œê°„ ì´ˆê³¼ (5ë¶„)")
        except Exception as e:
            raise InternalServerErrorException(detail=f"ìš”ì²­ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")

    async def _worker(self, worker_id: int):
        """
        ë³‘ë ¬ ì›Œì»¤ - ê° ì›Œì»¤ê°€ ë‹¨ì¼ íì—ì„œ ìš”ì²­ì„ ê°€ì ¸ì™€ ì²˜ë¦¬
        """
        print(f"{BLUE}INFO{RESET}:     Office Worker-{worker_id} ì‹œì‘ (ë‹¨ì¼ í)")
        
        # ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (ê° ì›Œì»¤ë³„ë¡œ)
        try:
            self.worker_models[worker_id] = LlamaOfficeModel()
            print(f"{GREEN}INFO{RESET}:     Office Worker-{worker_id} ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        except Exception as e:
            print(f"{RED}ERROR{RESET}:    Office Worker-{worker_id} ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
            return
        
        while self.is_running:
            try:
                # ë‹¨ì¼ íì—ì„œ ìš”ì²­ ê°€ì ¸ì˜¤ê¸° (íƒ€ì„ì•„ì›ƒ ì„¤ì •)
                request = await asyncio.wait_for(
                    self.request_queue.get(), 
                    timeout=1.0
                )
                
                actual_start_time = time.time()
                
                print(
                    f"ğŸ”„ Office Worker-{worker_id}: Processing {request.id[:8]} | "
                    f"User: {request.user_id}"
                )
                
                # ì‹¤ì œ ì²˜ë¦¬ (CPU ì§‘ì•½ì  ì‘ì—…ì„ ìŠ¤ë ˆë“œí’€ì—ì„œ ì‹¤í–‰)
                loop = asyncio.get_event_loop()
                result = await loop.run_in_executor(
                    None,
                    self.worker_models[worker_id].generate_response,
                    request.input_text,
                    request.search_text,
                    request.chat_list
                )
                
                # ê²°ê³¼ ê²€ì¦
                if not result or len(result.strip()) < 10:
                    print(f"âš ï¸  Warning: ì‘ë‹µì´ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤: '{result[:50]}...'")
                    result = "ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µì„ ì œëŒ€ë¡œ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”."
                
                # ê²°ê³¼ ì„¤ì •
                if not request.future.cancelled():
                    request.future.set_result(result)
                    self.total_processed += 1
                
                actual_processing_time = time.time() - actual_start_time
                total_time = time.time() - request.created_at
                
                print(
                    f"âœ… Office Worker-{worker_id}: Completed {request.id[:8]} | "
                    f"User: {request.user_id} | "
                    f"ProcessTime: {actual_processing_time:.3f}s | "
                    f"TotalTime: {total_time:.3f}s | "
                    f"ResponseLen: {len(result)} chars"
                )
                
            except asyncio.TimeoutError:
                # íƒ€ì„ì•„ì›ƒ - ê³„ì† ëŒ€ê¸°
                continue
            except asyncio.CancelledError:
                # ì •ìƒì ì¸ ì·¨ì†Œ
                break
            except Exception as e:
                self.total_errors += 1
                if 'request' in locals() and not request.future.cancelled():
                    request.future.set_exception(e)
                print(f"âŒ Office Worker-{worker_id}: Error processing request {request.id[:8]}: {e}")

    def get_queue_status(self) -> Dict[str, Any]:
        """í˜„ì¬ í ìƒíƒœ ë°˜í™˜"""
        return {
            "queue_size": self.request_queue.qsize(),
            "processing_mode": "single_queue",
            "is_running": self.is_running,
            "total_processed": self.total_processed,
            "total_errors": self.total_errors,
            "model_loaded": all(self.worker_models)
        }
    
    def get_performance_stats(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ í†µê³„ ë°˜í™˜"""
        avg_processing_time = 25.0  # ì˜ˆìƒ í‰ê·  ì²˜ë¦¬ ì‹œê°„
        if self.total_processed > 0:
            avg_processing_time = 25.0  # ì‹¤ì œ ì²˜ë¦¬ ì‹œê°„ ê¸°ì¤€ìœ¼ë¡œ ì¡°ì •
        
        estimated_wait_time = self.request_queue.qsize() * avg_processing_time / self.max_concurrent
        
        return {
            "total_processed": self.total_processed,
            "total_errors": self.total_errors,
            "success_rate": (
                (self.total_processed / (self.total_processed + self.total_errors)) * 100
                if (self.total_processed + self.total_errors) > 0 else 0
            ),
            "queue_size": self.request_queue.qsize(),
            "estimated_wait_time": f"{estimated_wait_time:.1f}s",
            "avg_processing_time": f"{avg_processing_time:.1f}s",
            "processing_mode": "single_queue",
            "worker_active": any(self.worker_models)
        }
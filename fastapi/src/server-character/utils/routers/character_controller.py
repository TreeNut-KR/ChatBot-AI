from fastapi import Path, APIRouter, HTTPException, Request
from fastapi.responses import JSONResponse
from pydantic import ValidationError

import time
import utils.app_state as AppState
from .. import OpenAiCharacter, ChatModel, ChatError

GREEN = "\033[32m"
RED = "\033[31m"
YELLOW = "\033[33m"
RESET = "\033[0m"

# ì²˜ë¦¬ ì‹œê°„ ì„ê³„ê°’ ì„¤ì • ìˆ˜ì •
MAX_PROCESSING_TIME = 180  # 3ë¶„ (180ì´ˆ)ë¡œ ì¤„ì„ - Nginx íƒ€ì„ì•„ì›ƒë³´ë‹¤ ì¶©ë¶„íˆ ì§§ê²Œ
RETRY_AFTER_MINUTES = 3    # 3ë¶„ í›„ ì¬ì‹œë„ ê¶Œì¥

OPENAI_MODEL_MAP = {
    "gpt4.1": {
        "id": "gpt-4.1",
        "type": "post"
    },
    "gpt4.1_mini": {
        "id": "gpt-4.1-mini",
        "type": "post"
    },
}

def get_openai_links(base_url: str, path: str) -> dict:
    """
    OPENAI_MODEL_MAPì„ ê¸°ì¤€ìœ¼ë¡œ _linksìš© ë§í¬ ë”•ì…”ë„ˆë¦¬ ìƒì„±
    """
    return [
        {
            "href": f"{base_url}character/{k}",
            "rel": f"character_{k}",
            "type": OPENAI_MODEL_MAP[k]["type"],
        }
        for k in OPENAI_MODEL_MAP.keys() if k != path
    ]

def calculate_estimated_time(queue_size: int) -> int:
    """
    í í¬ê¸°ì™€ ì›Œì»¤ 2ê°œ ê¸°ì¤€ìœ¼ë¡œ ì˜ˆìƒ ì²˜ë¦¬ ì‹œê°„ ê³„ì‚° (ì´ˆ ë‹¨ìœ„)
    - í™€ìˆ˜ ëŒ€ê¸°ë²ˆí˜¸: 20ì´ˆ ì²˜ë¦¬ (ì›Œì»¤1)
    - ì§ìˆ˜ ëŒ€ê¸°ë²ˆí˜¸: 10ì´ˆ ì²˜ë¦¬ (ì›Œì»¤2)
    - ì›Œì»¤ 2ê°œ ë³‘ë ¬ ì²˜ë¦¬
    
    ê° ì›Œì»¤ê°€ ë…ë¦½ì ìœ¼ë¡œ ëˆ„ì  ì²˜ë¦¬:
    í™€ìˆ˜ ì›Œì»¤: 20ì´ˆ, 40ì´ˆ, 60ì´ˆ, 80ì´ˆ, 100ì´ˆ, 120ì´ˆ...
    ì§ìˆ˜ ì›Œì»¤: 210ì´ˆ, 50ì´ˆ, 710ì´ˆ, 100ì´ˆ, 1210ì´ˆ, 150ì´ˆ...
    """
    if queue_size == 0:
        # íê°€ ë¹„ì–´ìˆìœ¼ë©´ ì²« ë²ˆì§¸ ìš”ì²­ (Position 1: 20ì´ˆ)
        return 20
    
    current_queue_position = queue_size + 1  # ìƒˆë¡œ ì¶”ê°€ë  ìš”ì²­ì˜ ìœ„ì¹˜
    
    # í™€ìˆ˜ ë²ˆí˜¸ (ì›Œì»¤1)
    if current_queue_position % 2 == 1:  
        worker_order = (current_queue_position + 1) // 2
        estimated_time = 20 * worker_order
    # ì§ìˆ˜ ë²ˆí˜¸ (ì›Œì»¤2)
    else:  
        worker_order = current_queue_position // 2
        estimated_time = 30 * worker_order
    
    return int(estimated_time)

character_router = APIRouter()

@character_router.post("/Llama", summary = "Llama ëª¨ë¸ì´ ì¼€ë¦­í„° ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€ ìƒì„±")
async def character_llama(request: ChatModel.character_Request, req: Request):
    """
    DarkIdol-Llama-3.1-8B ëª¨ë¸ì— ì§ˆë¬¸ì„ ì…ë ¥í•˜ê³  ìºë¦­í„° ì„¤ì •ì„ ë°˜ì˜í•˜ì—¬ ë‹µë³€ì„ JSON ë°©ì‹ìœ¼ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.

    Args:
        request (ChatModel.character_Request): ì‚¬ìš©ì ìš”ì²­ ë°ì´í„° í¬í•¨

    Returns:
        JSONResponse: JSON ë°©ì‹ìœ¼ë¡œ ëª¨ë¸ ì‘ë‹µ
    """
    chat_list = []
    start_time = time.time()
    queue_status_before = None
    performance_stats = None

    # MongoDBì—ì„œ ì±„íŒ… ê¸°ë¡ ê°€ì ¸ì˜¤ê¸°
    if AppState.mongo_handler and request.db_id:
        try:
            chat_list = await AppState.mongo_handler.get_character_log(
                user_id = request.user_id,
                document_id = request.db_id,
                router = "character",
            )
        except Exception as e:
            print(f"{YELLOW}WARNING{RESET}:  ì±„íŒ… ê¸°ë¡ì„ ê°€ì ¸ì˜¤ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {str(e)}")
            
    try:
        character_settings = {
            "character_name": request.character_name,
            "greeting": request.greeting,
            "context": request.context,
            "chat_list": chat_list,
        }
        
        # í ìƒíƒœ í™•ì¸ ë° ì˜ˆìƒ ëŒ€ê¸° ì‹œê°„ ê³„ì‚°
        queue_status_before = AppState.llama_queue_handler.get_queue_status()
        performance_stats = AppState.llama_queue_handler.get_performance_stats()

        # í í¬ê¸° (ë‹¨ì¼ íë¡œ ë³€ê²½)
        total_queue_size = queue_status_before['queue_size']
        
        # ì˜ˆìƒ ì²˜ë¦¬ ì‹œê°„ ê³„ì‚° (ì›Œì»¤ 2ê°œ, í™€ìˆ˜ 20ì´ˆ/ì§ìˆ˜ 10ì´ˆ)
        estimated_time_seconds = calculate_estimated_time(total_queue_size)
        
        # ì˜ˆìƒ ì²˜ë¦¬ ì‹œê°„ì´ 180ì´ˆ(3ë¶„) ì´ìƒì´ë©´ ë¯¸ë¦¬ 429 ë°˜í™˜
        if estimated_time_seconds > MAX_PROCESSING_TIME:
            estimated_minutes = estimated_time_seconds // 60
            retry_after_seconds = RETRY_AFTER_MINUTES * 60 + 10
            
            current_position = total_queue_size + 1
            worker_type = "í™€ìˆ˜(20ì´ˆ)" if current_position % 2 == 1 else "ì§ìˆ˜(10ì´ˆ)"
            
            print(
                f"{YELLOW}â° PRE-TIMEOUT{RESET}: Character: {request.character_name} | User: {request.user_id} | "
                f"Queue: {total_queue_size} | Position: {current_position}({worker_type}) | "
                f"Estimated: {estimated_time_seconds}s ({estimated_minutes}ë¶„) | "
                f"Threshold: {MAX_PROCESSING_TIME}s | HTTP: 429"
            )
        
            return JSONResponse(
                status_code=429,
                content={
                    "error": "Too Many Requests",
                    "message": f"í˜„ì¬ ì„œë²„ ì´ìš©ìê°€ ë§ì•„ ì˜ˆìƒ ì²˜ë¦¬ ì‹œê°„ì´ {estimated_minutes}ë¶„ì…ë‹ˆë‹¤. {RETRY_AFTER_MINUTES}ë¶„ í›„ ë‹¤ì‹œ ìš”ì²­í•´ ì£¼ì„¸ìš”.",
                    "code": "QUEUE_OVERLOADED",
                    "retry_after": retry_after_seconds,
                    "queue_info": {
                        "current_queue_size": total_queue_size,
                        "your_position": current_position,
                        "worker_type": worker_type,
                        "estimated_wait_time_seconds": estimated_time_seconds,
                        "estimated_wait_time_minutes": estimated_minutes,
                        "max_allowed_time_seconds": MAX_PROCESSING_TIME,  # 180ì´ˆë¡œ ì—…ë°ì´íŠ¸
                        "processing_model": "dual_worker_odd_even"
                    }
                },
                headers={"Retry-After": str(retry_after_seconds)}
            )

        current_position = total_queue_size + 1
        worker_type = "í™€ìˆ˜(20ì´ˆ)" if current_position % 2 == 1 else "ì§ìˆ˜(10ì´ˆ)"
        
        print(
            f"ğŸ”„ íì— ìš”ì²­ ì¶”ê°€: Character: {request.character_name} | User: {request.user_id} | "
            f"Queue: {total_queue_size} | Position: {current_position}({worker_type}) | "
            f"ì˜ˆìƒ ëŒ€ê¸°: {estimated_time_seconds}s ({estimated_time_seconds//60}ë¶„ {estimated_time_seconds%60}ì´ˆ) | "
            f"InputLen: {len(request.input_data)} chars"
        )
        
        full_response = await AppState.llama_queue_handler.add_request(
            input_text=request.input_data,
            character_settings=character_settings,
            user_id=request.user_id,
            character_name=request.character_name
        )
        
        processing_time = time.time() - start_time
        
        response_data = {
            "result": full_response,
            "processing_info": {
                "processing_time": f"{processing_time:.3f}s",
                "queue_position_before": total_queue_size,
                "your_position": total_queue_size + 1,
                "worker_type": "í™€ìˆ˜(20ì´ˆ)" if (total_queue_size + 1) % 2 == 1 else "ì§ìˆ˜(10ì´ˆ)",
                "estimated_wait_time": f"{estimated_time_seconds}s",
                "processing_mode": "dual_worker_odd_even"
            },
            "_links": [
                {   
                    "href": str(req.url),
                    "rel": "_self",
                    "type": str(req.method).lower(),
                },
                *get_openai_links(
                    base_url = str(req.base_url),
                    path = "Llama",
                ),
            ]
        }
        return JSONResponse(content=response_data)

    except TimeoutError as te:
        return JSONResponse(
            status_code=429,
            content={
                "error": "Too Many Requests", 
                "message": "ì„œë²„ ì´ìš©ìê°€ ë§ì•„ ì²˜ë¦¬ ì‹œê°„ì´ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ìš”ì²­í•´ ì£¼ì„¸ìš”.",
                "code": "EXPLICIT_TIMEOUT_ERROR",
                "retry_after": retry_after_seconds,
                "queue_info": {
                    "current_queue_size": queue_status_before.get('queue_size', 0) if queue_status_before else 0,
                    "estimated_wait_time": performance_stats.get('estimated_wait_time', 'ì•Œ ìˆ˜ ì—†ìŒ') if performance_stats else 'ì•Œ ìˆ˜ ì—†ìŒ'
                }
            },
            headers={"Retry-After": str(retry_after_seconds)}
        )
    except ValidationError as ve:
        raise ChatError.BadRequestException(detail=str(ve))
    except Exception as e:
        error_str = str(e)
        timeout_keywords = [
            "ì‹œê°„ ì´ˆê³¼", "timeout", "ìš”ì²­ ì²˜ë¦¬ ì‹œê°„ ì´ˆê³¼", "5ë¶„", "4ë¶„",
            "TimeoutError", "asyncio.TimeoutError", "concurrent.futures.TimeoutError",
            "ì²˜ë¦¬ ì‹œê°„ì´ ì´ˆê³¼", "time limit exceeded", "request timeout", "500:", "504:"
        ]
        is_timeout = any(keyword in error_str.lower() for keyword in [kw.lower() for kw in timeout_keywords])
        
        if is_timeout or "ì‹œê°„ ì´ˆê³¼" in error_str:
            return JSONResponse(
                status_code=429,
                content={
                    "error": "Too Many Requests",
                    "message": "ì„œë²„ ì´ìš©ìê°€ ë§ì•„ ì²˜ë¦¬ ì‹œê°„ì´ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ìš”ì²­í•´ ì£¼ì„¸ìš”.",
                    "code": "TIMEOUT_DUE_TO_HIGH_LOAD",
                    "retry_after": retry_after_seconds,
                    "debug_info": {
                        "original_error": error_str,
                        "detected_as": "timeout_error",
                        "detection_method": "keyword_matching"
                    }
                },
                headers={"Retry-After": str(retry_after_seconds)}
            )
        else:
            raise ChatError.InternalServerErrorException(detail="ë‚´ë¶€ ì„œë²„ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")

@character_router.post("/{gpt_set}", summary = "gpt ëª¨ë¸ì´ ì¼€ë¦­í„° ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€ ìƒì„±")
async def character_gpt4o_mini(
        request: ChatModel.character_Request,
        req: Request,
        gpt_set: str = Path(
            ...,
            title="GPT ëª¨ë¸ëª…",
            description= f"ì‚¬ìš©í•  OpenAI GPT ëª¨ë¸ì˜ ë³„ì¹­ (ì˜ˆ: {list(OPENAI_MODEL_MAP.keys())})",
        )
    ):
    """
    gpt ëª¨ë¸ì— ì§ˆë¬¸ì„ ì…ë ¥í•˜ê³  ìºë¦­í„° ì„¤ì •ì„ ë°˜ì˜í•˜ì—¬ ë‹µë³€ì„ JSON ë°©ì‹ìœ¼ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.

    Args:
        request (ChatModel.character_Request): ì‚¬ìš©ì ìš”ì²­ ë°ì´í„° í¬í•¨

    Returns:
        JSONResponse: JSON ë°©ì‹ìœ¼ë¡œ ëª¨ë¸ ì‘ë‹µ
    """
    if gpt_set not in OPENAI_MODEL_MAP:
        raise HTTPException(status_code = 400, detail = "Invalid model name.")

    model_id = OPENAI_MODEL_MAP[gpt_set]["id"]
    chat_list = []
    
    # MongoDBì—ì„œ ì±„íŒ… ê¸°ë¡ ê°€ì ¸ì˜¤ê¸°
    if AppState.mongo_handler and request.db_id:
        try:
            chat_list = await AppState.mongo_handler.get_character_log(
                user_id = request.user_id,
                document_id = request.db_id,
                router = "character",
            )
        except Exception as e:
            print(f"{YELLOW}WARNING{RESET}:  ì±„íŒ… ê¸°ë¡ì„ ê°€ì ¸ì˜¤ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {str(e)}")

    OpenAiCharacter_model = OpenAiCharacter(model_id = model_id)
    try:
        character_settings = {
            "character_name": request.character_name,
            "greeting": request.greeting,
            "context": request.context,
            "chat_list": chat_list,
        }
        full_response = OpenAiCharacter_model.generate_response(
            input_text =  request.input_data,
            character_settings = character_settings,
        )
        response_data = {
            "result": full_response,
            "_links": [
                {
                    "href": str(req.url),
                    "rel": "_self",
                    "type": str(req.method).lower(),
                },
                {
                    "href": str(req.base_url) + "character/Llama",
                    "rel": "character_llama",
                    "type": "post",
                },
                *get_openai_links(
                    base_url = str(req.base_url),
                    path = gpt_set,
                ),
            ]
        }
        return JSONResponse(content=response_data)

    except TimeoutError as te:
        return JSONResponse(
            status_code=429,
            content={
                "error": "Too Many Requests",
                "message": "ì„œë²„ ì´ìš©ìê°€ ë§ì•„ ì²˜ë¦¬ ì‹œê°„ì´ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ìš”ì²­í•´ ì£¼ì„¸ìš”.",
                "code": "GPT_EXPLICIT_TIMEOUT_ERROR",
                "retry_after": 60
            },
            headers={"Retry-After": "60"}
        )
    except ValidationError as ve:
        raise ChatError.BadRequestException(detail=str(ve))
    except Exception as e:
        error_str = str(e)
        timeout_keywords = [
            "ì‹œê°„ ì´ˆê³¼", "timeout", "ìš”ì²­ ì²˜ë¦¬ ì‹œê°„ ì´ˆê³¼", "5ë¶„", "4ë¶„",
            "TimeoutError", "asyncio.TimeoutError", "concurrent.futures.TimeoutError",
            "ì²˜ë¦¬ ì‹œê°„ì´ ì´ˆê³¼", "time limit exceeded", "request timeout", "500:", "504:"
        ]
        
        is_timeout = any(keyword in error_str.lower() for keyword in [kw.lower() for kw in timeout_keywords])
        
        if is_timeout or "500: ìš”ì²­ ì²˜ë¦¬ ì‹œê°„ ì´ˆê³¼" in error_str:
            return JSONResponse(
                status_code=429,
                content={
                    "error": "Too Many Requests",
                    "message": "ì„œë²„ ì´ìš©ìê°€ ë§ì•„ ì²˜ë¦¬ ì‹œê°„ì´ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ìš”ì²­í•´ ì£¼ì„¸ìš”.",
                    "code": "GPT_TIMEOUT_DUE_TO_HIGH_LOAD",
                    "retry_after": 60,
                    "debug_info": {
                        "original_error": error_str,
                        "detected_as": "timeout_error",
                        "detection_method": "keyword_matching"
                    }
                },
                headers={"Retry-After": "60"}
            )
        else:
            raise ChatError.InternalServerErrorException(detail="ë‚´ë¶€ ì„œë²„ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
        
@character_router.get("/performance", summary="ì„±ëŠ¥ í†µê³„ ì¡°íšŒ")
async def get_performance():
    """
    í í•¸ë“¤ëŸ¬ì˜ ì„±ëŠ¥ í†µê³„ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    if AppState.llama_queue_handler:
        stats = AppState.llama_queue_handler.get_performance_stats()
        return JSONResponse(content=stats)
    else:
        raise ChatError.InternalServerErrorException(detail="í í•¸ë“¤ëŸ¬ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

# ğŸ“‚ ./fastapi/ai_model **í´ë”**

> í•´ë‹¹ í´ë”ëŠ” ì‚¬ìš© ì¤‘ì¸ ëª¨ë¸ì— ëŒ€í•œ ìë£Œë¥¼ ë‹´ëŠ” í´ë”ì´ë‹¤.
>
> modelì˜ ì‚¬ìš© ë°©í–¥ì´ ë‹¤ë¥´ê¸° ë•Œë¬¸ì— ê¸°ë³¸ Llamaì™€ Llama ê¸°ë°˜ì˜ Lumimaid, Bllossom GGUF ëª¨ë¸ì„ ì‚¬ìš© ì¤‘.
>

### ğŸŸ¢ **ëª¨ë¸ ì„¤ëª…**

| í•­ëª© | **LlamaChatModel** | **LumimaidChatModel** | **BllossomChatModel** |
|------|--------------------|----------------------|-----------------------|
| **ê¸°ë°˜ ëª¨ë¸** | Llama-3.1-8B-Instruct | Llama-3-Lumimaid-8B | Llama-3-Korean-Bllossom-8B |
| **ì œì‘ì** | Meta | Lewdiculous | MLP-KTLim |
| **í¬ë§·** | í‘œì¤€ Hugging Face Transformers ëª¨ë¸ | GGUF í¬ë§· (ì••ì¶•, ê²½ëŸ‰í™”) | GGUF í¬ë§· (ì••ì¶•, ê²½ëŸ‰í™”) |
| **ì¥ì¹˜ í™œìš©** | `torch.device("cuda:0")` | `gpu_layers`ë¥¼ ì´ìš©í•´ GPU í• ë‹¹ | `n_gpu_layers`ë¥¼ ì´ìš©í•´ GPU í• ë‹¹ |
| **ì–‘ìí™” ì„¤ì •** | `BitsAndBytesConfig`ë¡œ 4bit ì–‘ìí™” | GGUF ìì²´ì˜ ì–‘ìí™”ëœ ëª¨ë¸ ì‚¬ìš© | GGUF ìì²´ì˜ ì–‘ìí™”ëœ ëª¨ë¸ ì‚¬ìš© |
| **ë¡œë”© ë°©ì‹** | `transformers`ì˜ `AutoModelForCausalLM` | `llama_cpp_cuda` | `llama_cpp_cuda` |
| **ì‚¬ì´íŠ¸** | [meta-llama/Llama-3.1-8B-Instruct](https://huggingface.co/meta-llama/Llama-3.1-8B) | [Lewdiculous/Llama-3-Lumimaid-8B](https://huggingface.co/Lewdiculous/Llama-3-Lumimaid-8B-v0.1-OAS-GGUF-IQ-Imatrix) | [MLP-KTLim/llama-3-Korean-Bllossom-8B-gguf-Q4_K_M](https://huggingface.co/MLP-KTLim/llama-3-Korean-Bllossom-8B-gguf-Q4_K_M) |

